
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>pyvbmc.vbmc.variational_optimization &#8212; PyVBMC</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="canonical" href="https://acerbilab.github.io/pyvbmc/_modules/pyvbmc/vbmc/variational_optimization.html" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/logo.svg" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../quickstart.html">
   Getting started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../api/classes/vbmc.html">
   VBMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../api/classes/variational_posterior.html">
   VariationalPosterior
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../api/options/vbmc_options.html">
   VBMC options
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../api/advanced_docs.html">
   Advanced documentation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/classes/acquisition_functions.html">
     Acquisition Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/classes/function_logger.html">
     FunctionLogger
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/classes/iteration_history.html">
     IterationHistory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/classes/options.html">
     Options
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/classes/parameter_transformer.html">
     ParameterTransformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/classes/timer.html">
     Timer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/classes/variational_posterior.html">
     VariationalPosterior
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/classes/vbmc.html">
     VBMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/functions/active_sample.html">
     active_sample
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/functions/create_vbmc_animation.html">
     create_vbmc_animation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/functions/decorators.html">
     decorators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/functions/entropy.html">
     entropy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/functions/get_hpd.html">
     get_hpd
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/functions/kde_1d.html">
     kde_1d
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/functions/whitening.html">
     whitening
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../api/options/vbmc_options.html">
     VBMC options
    </a>
   </li>
  </ul>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../_examples/pyvbmc_example_1_basic_usage.html">
   PyVBMC Example 1: Basic usage
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../_examples/pyvbmc_example_2_inputs_outputs.html">
   PyVBMC Example 2: Understanding the inputs and the output trace
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../_examples/pyvbmc_example_3_diagnostics_and_saving.html">
   PyVBMC Example 3: Output diagnostics and saving results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../_examples/pyvbmc_example_4_validation.html">
   PyVBMC Example 4: Multiple runs as validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../_examples/pyvbmc_example_5_noisy_likelihoods.html">
   PyVBMC Example 5: Noisy log-likelihood evaluations
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../development.html">
   Instructions for developers and contributors
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../about_us.html">
   About us
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/acerbilab/pyvbmc"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/acerbilab/pyvbmc/issues/new?title=Issue%20on%20page%20%2F_modules/pyvbmc/vbmc/variational_optimization.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for pyvbmc.vbmc.variational_optimization</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Variational optimization / training of variational posterior&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">gpyreg</span> <span class="k">as</span> <span class="nn">gpr</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>

<span class="kn">from</span> <span class="nn">pyvbmc.entropy</span> <span class="kn">import</span> <span class="n">entlb_vbmc</span><span class="p">,</span> <span class="n">entmc_vbmc</span>
<span class="kn">from</span> <span class="nn">pyvbmc.stats</span> <span class="kn">import</span> <span class="n">get_hpd</span>
<span class="kn">from</span> <span class="nn">pyvbmc.variational_posterior</span> <span class="kn">import</span> <span class="n">VariationalPosterior</span>

<span class="kn">from</span> <span class="nn">.iteration_history</span> <span class="kn">import</span> <span class="n">IterationHistory</span>
<span class="kn">from</span> <span class="nn">.minimize_adam</span> <span class="kn">import</span> <span class="n">minimize_adam</span>
<span class="kn">from</span> <span class="nn">.options</span> <span class="kn">import</span> <span class="n">Options</span>


<div class="viewcode-block" id="update_K"><a class="viewcode-back" href="../../../api/classes/vbmc.html#pyvbmc.vbmc.update_K">[docs]</a><span class="k">def</span> <span class="nf">update_K</span><span class="p">(</span>
    <span class="n">optim_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">iteration_history</span><span class="p">:</span> <span class="n">IterationHistory</span><span class="p">,</span> <span class="n">options</span><span class="p">:</span> <span class="n">Options</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update number of variational mixture components.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ==========</span>
<span class="sd">    optim_state : dict</span>
<span class="sd">        Optimization state from the VBMC instance we are calling this from.</span>
<span class="sd">    iteration_history : IterationHistory</span>
<span class="sd">        Iteration history from the VBMC instance we are calling this from.</span>
<span class="sd">    options : Options</span>
<span class="sd">        Options from the VBMC instance we are calling this from.</span>

<span class="sd">    Returns</span>
<span class="sd">    =======</span>
<span class="sd">    K_new : int</span>
<span class="sd">        The new number of variational mixture components.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">K_new</span> <span class="o">=</span> <span class="n">optim_state</span><span class="p">[</span><span class="s2">&quot;vp_K&quot;</span><span class="p">]</span>

    <span class="c1"># Compute maximum number of components</span>
    <span class="n">K_max</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">options</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="s2">&quot;k_fun_max&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;N&quot;</span><span class="p">:</span> <span class="n">optim_state</span><span class="p">[</span><span class="s2">&quot;n_eff&quot;</span><span class="p">]}))</span>

    <span class="c1"># Evaluate bonus for stable solution.</span>
    <span class="n">K_bonus</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">options</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="s2">&quot;adaptive_k&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;unkn&quot;</span><span class="p">:</span> <span class="n">K_new</span><span class="p">}))</span>

    <span class="c1"># If not warming up, check if number of components gets to be increased.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">optim_state</span><span class="p">[</span><span class="s2">&quot;warmup&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">optim_state</span><span class="p">[</span><span class="s2">&quot;iter&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">recent_iters</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span>
            <span class="mf">0.5</span> <span class="o">*</span> <span class="n">options</span><span class="p">[</span><span class="s2">&quot;tol_stable_count&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">options</span><span class="p">[</span><span class="s2">&quot;fun_evals_per_iter&quot;</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># Check if ELCBO has improved wrt recent iterations</span>
        <span class="n">lower_end</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">optim_state</span><span class="p">[</span><span class="s2">&quot;iter&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">recent_iters</span><span class="p">)</span>
        <span class="n">elbos</span> <span class="o">=</span> <span class="n">iteration_history</span><span class="p">[</span><span class="s2">&quot;elbo&quot;</span><span class="p">][</span><span class="n">lower_end</span><span class="p">:]</span>
        <span class="n">elboSDs</span> <span class="o">=</span> <span class="n">iteration_history</span><span class="p">[</span><span class="s2">&quot;elbo_sd&quot;</span><span class="p">][</span><span class="n">lower_end</span><span class="p">:]</span>
        <span class="n">elcbos</span> <span class="o">=</span> <span class="n">elbos</span> <span class="o">-</span> <span class="n">options</span><span class="p">[</span><span class="s2">&quot;elcbo_impro_weight&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">elboSDs</span>
        <span class="n">warmups</span> <span class="o">=</span> <span class="n">iteration_history</span><span class="p">[</span><span class="s2">&quot;warmup&quot;</span><span class="p">][</span><span class="n">lower_end</span><span class="p">:]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
        <span class="n">elcbos_after</span> <span class="o">=</span> <span class="n">elcbos</span><span class="p">[</span><span class="o">~</span><span class="n">warmups</span><span class="p">]</span>
        <span class="c1"># Ignore two iterations right after warmup.</span>
        <span class="n">elcbos_after</span><span class="p">[</span><span class="mi">0</span> <span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">optim_state</span><span class="p">[</span><span class="s2">&quot;iter&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="n">elcbo_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">elcbos_after</span><span class="p">)</span>
        <span class="n">improving_flag</span> <span class="o">=</span> <span class="n">elcbos_after</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">elcbo_max</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span>
            <span class="n">elcbos_after</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># Add one component if ELCBO is improving and no pruning in</span>
        <span class="c1"># the last iteration</span>
        <span class="k">if</span> <span class="n">iteration_history</span><span class="p">[</span><span class="s2">&quot;pruned&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">improving_flag</span><span class="p">:</span>
            <span class="n">K_new</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Bonus components for stable solution (speed up exploration)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">iteration_history</span><span class="p">[</span><span class="s2">&quot;r_index&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="n">optim_state</span><span class="p">[</span><span class="s2">&quot;recompute_var_post&quot;</span><span class="p">]</span>
            <span class="ow">and</span> <span class="n">improving_flag</span>
        <span class="p">):</span>
            <span class="c1"># No bonus if any component was very recently pruned.</span>
            <span class="n">new_lower_end</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="n">optim_state</span><span class="p">[</span><span class="s2">&quot;iter&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">recent_iters</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">iteration_history</span><span class="p">[</span><span class="s2">&quot;pruned&quot;</span><span class="p">][</span><span class="n">new_lower_end</span><span class="p">:]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
                <span class="n">K_new</span> <span class="o">+=</span> <span class="n">K_bonus</span>

        <span class="n">K_new</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">optim_state</span><span class="p">[</span><span class="s2">&quot;vp_K&quot;</span><span class="p">],</span> <span class="nb">min</span><span class="p">(</span><span class="n">K_new</span><span class="p">,</span> <span class="n">K_max</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">K_new</span></div>


<div class="viewcode-block" id="optimize_vp"><a class="viewcode-back" href="../../../api/classes/vbmc.html#pyvbmc.vbmc.optimize_vp">[docs]</a><span class="k">def</span> <span class="nf">optimize_vp</span><span class="p">(</span>
    <span class="n">options</span><span class="p">:</span> <span class="n">Options</span><span class="p">,</span>
    <span class="n">optim_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
    <span class="n">vp</span><span class="p">:</span> <span class="n">VariationalPosterior</span><span class="p">,</span>
    <span class="n">gp</span><span class="p">:</span> <span class="n">gpr</span><span class="o">.</span><span class="n">GP</span><span class="p">,</span>
    <span class="n">fast_opts_N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">slow_opts_N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">K</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimize variational posterior.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ==========</span>
<span class="sd">    options : Options</span>
<span class="sd">        Options from the VBMC instance we are calling this from.</span>
<span class="sd">    optim_state : dict</span>
<span class="sd">        Optimization state from the VBMC instance we are calling this from.</span>
<span class="sd">    vp : VariationalPosterior</span>
<span class="sd">        The variational posterior we want to optimize.</span>
<span class="sd">    gp : gpyreg.GaussianProcess</span>
<span class="sd">        The Gaussian process surrogate of the log-posterior, against which to</span>
<span class="sd">        optimize the VP.</span>
<span class="sd">    fast_opts_N : int</span>
<span class="sd">        Number of fast optimizations.</span>
<span class="sd">    slow_opts_N : int</span>
<span class="sd">        Number of slow optimizations.</span>
<span class="sd">    K : int, optional</span>
<span class="sd">        Number of mixture components. If not given defaults to the number</span>
<span class="sd">        of mixture components the given VP has.</span>

<span class="sd">    Returns</span>
<span class="sd">    =======</span>
<span class="sd">    vp : VariationalPosterior</span>
<span class="sd">        The optimized variational posterior.</span>
<span class="sd">    var_ss : int</span>
<span class="sd">        Estimated variance of the ELBO, due to variance of the expected</span>
<span class="sd">        log-joint, for each GP hyperparameter sample.</span>
<span class="sd">    pruned : int</span>
<span class="sd">        Number of pruned components.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">K</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">K</span>

    <span class="c1"># Missing port: assigning default values to options if it is none</span>
    <span class="c1">#               due to the new structure of the program</span>

    <span class="c1"># Missing port: assign default values to optim_state since these are</span>
    <span class="c1">#               not really used</span>

    <span class="c1"># Turn off weight optimization in warm up if not already done.</span>
    <span class="k">if</span> <span class="n">optim_state</span><span class="p">[</span><span class="s2">&quot;warmup&quot;</span><span class="p">]:</span>
        <span class="n">vp</span><span class="o">.</span><span class="n">optimize_weights</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># Quick sieve optimization to determine starting point(s)</span>
    <span class="n">vp0_vec</span><span class="p">,</span> <span class="n">vp0_type</span><span class="p">,</span> <span class="n">elcbo_beta</span><span class="p">,</span> <span class="n">compute_var</span><span class="p">,</span> <span class="n">ns_ent_K</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_sieve</span><span class="p">(</span>
        <span class="n">options</span><span class="p">,</span>
        <span class="n">optim_state</span><span class="p">,</span>
        <span class="n">vp</span><span class="p">,</span>
        <span class="n">gp</span><span class="p">,</span>
        <span class="n">K</span><span class="o">=</span><span class="n">K</span><span class="p">,</span>
        <span class="n">init_N</span><span class="o">=</span><span class="n">fast_opts_N</span><span class="p">,</span>
        <span class="n">best_N</span><span class="o">=</span><span class="n">slow_opts_N</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Compute soft bounds for variational parameter optimization.</span>
    <span class="n">theta_bnd</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">get_bounds</span><span class="p">(</span><span class="n">gp</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">options</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>

    <span class="c1">## Perform optimization starting from one or few selected points.</span>

    <span class="c1"># Set up an empty stats struct for optimization</span>
    <span class="n">theta_N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">vp0_vec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>
    <span class="n">Ns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">gp</span><span class="o">.</span><span class="n">posteriors</span><span class="p">)</span>
    <span class="n">elbo_stats</span> <span class="o">=</span> <span class="n">_initialize_full_elcbo</span><span class="p">(</span><span class="n">slow_opts_N</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">theta_N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">Ns</span><span class="p">)</span>

    <span class="c1"># For the moment no gradient available for variance</span>
    <span class="n">gradient_available</span> <span class="o">=</span> <span class="n">compute_var</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">gradient_available</span><span class="p">:</span>
        <span class="c1"># Set basic options for deterministic (?) optimizer</span>
        <span class="n">compute_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">ns_ent_K</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sd">&quot;&quot;&quot;Gradients must be available when ns_ent_K is &gt; 0.&quot;&quot;&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">compute_grad</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="n">vp0_fine</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">slow_opts_N</span><span class="p">):</span>
        <span class="c1"># Be careful with off-by-one errors here, Python is zero-based.</span>
        <span class="n">i_mid</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">2</span>
        <span class="n">i_end</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># Select points from best ones depending on subset</span>
        <span class="k">if</span> <span class="n">slow_opts_N</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="n">slow_opts_N</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">vp0_type</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">vp0_type</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">vp0_type</span> <span class="o">==</span> <span class="mi">3</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">vp0_type</span> <span class="o">==</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">vp0</span> <span class="o">=</span> <span class="n">vp0_vec</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">vp0_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">vp0_vec</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
        <span class="n">vp0_type</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">vp0_type</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
        <span class="n">theta0</span> <span class="o">=</span> <span class="n">vp0</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">ns_ent_K</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Fast optimization via deterministic entropy approximation</span>

            <span class="c1"># Objective function</span>
            <span class="k">def</span> <span class="nf">vb_train_fun</span><span class="p">(</span><span class="n">theta_</span><span class="p">):</span>
                <span class="n">res</span> <span class="o">=</span> <span class="n">_neg_elcbo</span><span class="p">(</span>
                    <span class="n">theta_</span><span class="p">,</span>
                    <span class="n">gp</span><span class="p">,</span>
                    <span class="n">vp0</span><span class="p">,</span>
                    <span class="n">elcbo_beta</span><span class="p">,</span>
                    <span class="mi">0</span><span class="p">,</span>
                    <span class="n">compute_grad</span><span class="o">=</span><span class="n">compute_grad</span><span class="p">,</span>
                    <span class="n">compute_var</span><span class="o">=</span><span class="n">compute_var</span><span class="p">,</span>
                    <span class="n">theta_bnd</span><span class="o">=</span><span class="n">theta_bnd</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">compute_grad</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">res</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">res</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
                <span class="n">vb_train_fun</span><span class="p">,</span>
                <span class="n">theta0</span><span class="p">,</span>
                <span class="n">jac</span><span class="o">=</span><span class="n">compute_grad</span><span class="p">,</span>
                <span class="n">tol</span><span class="o">=</span><span class="n">options</span><span class="p">[</span><span class="s2">&quot;det_entropy_tol_opt&quot;</span><span class="p">],</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">res</span><span class="o">.</span><span class="n">success</span><span class="p">:</span>
                <span class="c1"># SciPy minimize failed</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot optimize variational parameters with&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;scipy.optimize.minimize.&quot;</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">theta_opt</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Objective function, should only return value and gradient.</span>
            <span class="k">def</span> <span class="nf">vb_train_mc_fun</span><span class="p">(</span><span class="n">theta_</span><span class="p">):</span>
                <span class="n">res</span> <span class="o">=</span> <span class="n">_neg_elcbo</span><span class="p">(</span>
                    <span class="n">theta_</span><span class="p">,</span>
                    <span class="n">gp</span><span class="p">,</span>
                    <span class="n">vp0</span><span class="p">,</span>
                    <span class="n">elcbo_beta</span><span class="p">,</span>
                    <span class="n">ns_ent_K</span><span class="p">,</span>
                    <span class="n">compute_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">compute_var</span><span class="o">=</span><span class="n">compute_var</span><span class="p">,</span>
                    <span class="n">theta_bnd</span><span class="o">=</span><span class="n">theta_bnd</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">res</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="c1"># Optimization via unbiased stochastic entroply approximation</span>
            <span class="n">theta_opt</span> <span class="o">=</span> <span class="n">theta0</span>

            <span class="k">if</span> <span class="n">options</span><span class="p">[</span><span class="s2">&quot;stochastic_optimizer&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;adam&quot;</span><span class="p">:</span>
                <span class="n">master_min</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">options</span><span class="p">[</span><span class="s2">&quot;sgd_step_size&quot;</span><span class="p">],</span> <span class="mf">0.001</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">optim_state</span><span class="p">[</span><span class="s2">&quot;warmup&quot;</span><span class="p">]</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_weights</span><span class="p">:</span>
                    <span class="n">scaling_factor</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">options</span><span class="p">[</span><span class="s2">&quot;sgd_step_size&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">scaling_factor</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">options</span><span class="p">[</span><span class="s2">&quot;sgd_step_size&quot;</span><span class="p">])</span>

                <span class="c1"># Fixed master stepsize</span>
                <span class="n">master_max</span> <span class="o">=</span> <span class="n">scaling_factor</span>

                <span class="c1"># Note: we tried to adapt the stepsizes guided by the GP</span>
                <span class="c1"># hyperparameters, but this did not seem to help (the former</span>
                <span class="c1"># experimental option was &quot;GPStochasticStepsize&quot;).</span>
                <span class="n">master_max</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">master_min</span><span class="p">,</span> <span class="n">master_max</span><span class="p">)</span>
                <span class="n">master_decay</span> <span class="o">=</span> <span class="mi">200</span>
                <span class="n">max_iter</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="n">options</span><span class="p">[</span><span class="s2">&quot;max_iter_stochastic&quot;</span><span class="p">])</span>

                <span class="n">theta_opt</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">theta_lst</span><span class="p">,</span> <span class="n">f_val_lst</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">minimize_adam</span><span class="p">(</span>
                    <span class="n">vb_train_mc_fun</span><span class="p">,</span>
                    <span class="n">theta_opt</span><span class="p">,</span>
                    <span class="n">tol_fun</span><span class="o">=</span><span class="n">options</span><span class="p">[</span><span class="s2">&quot;tol_fun_stochastic&quot;</span><span class="p">],</span>
                    <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
                    <span class="n">master_min</span><span class="o">=</span><span class="n">master_min</span><span class="p">,</span>
                    <span class="n">master_max</span><span class="o">=</span><span class="n">master_max</span><span class="p">,</span>
                    <span class="n">master_decay</span><span class="o">=</span><span class="n">master_decay</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="n">options</span><span class="p">[</span><span class="s2">&quot;elcbo_midpoint&quot;</span><span class="p">]:</span>
                    <span class="c1"># Recompute ELCBO at best midpoint with full variance</span>
                    <span class="c1"># and more precision.</span>
                    <span class="n">idx_mid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">f_val_lst</span><span class="p">)</span>
                    <span class="n">elbo_stats</span> <span class="o">=</span> <span class="n">_eval_full_elcbo</span><span class="p">(</span>
                        <span class="n">i_mid</span><span class="p">,</span>
                        <span class="n">theta_lst</span><span class="p">[:,</span> <span class="n">idx_mid</span><span class="p">],</span>
                        <span class="n">vp0</span><span class="p">,</span>
                        <span class="n">gp</span><span class="p">,</span>
                        <span class="n">elbo_stats</span><span class="p">,</span>
                        <span class="n">elcbo_beta</span><span class="p">,</span>
                        <span class="n">options</span><span class="p">,</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unknown stochastic optimizer!&quot;</span><span class="p">)</span>

        <span class="c1"># Recompute ELCBO at endpoint with full variance and more precision</span>
        <span class="n">elbo_stats</span> <span class="o">=</span> <span class="n">_eval_full_elcbo</span><span class="p">(</span>
            <span class="n">i_end</span><span class="p">,</span> <span class="n">theta_opt</span><span class="p">,</span> <span class="n">vp0</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">elbo_stats</span><span class="p">,</span> <span class="n">elcbo_beta</span><span class="p">,</span> <span class="n">options</span>
        <span class="p">)</span>

        <span class="n">vp0_fine</span><span class="p">[</span><span class="n">i_mid</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">vp0</span><span class="p">)</span>
        <span class="n">vp0_fine</span><span class="p">[</span><span class="n">i_end</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">vp0</span><span class="p">)</span>  <span class="c1"># Parameters get assigned later</span>

    <span class="c1">## Finalize optimization by taking variational parameters with best ELCBO</span>

    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;nelcbo&quot;</span><span class="p">])</span>
    <span class="n">elbo</span> <span class="o">=</span> <span class="o">-</span><span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;nelbo&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">elbo_sd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;varF&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">])</span>
    <span class="n">G</span> <span class="o">=</span> <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;G&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;H&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">var_ss</span> <span class="o">=</span> <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;var_ss&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">varG</span> <span class="o">=</span> <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;varG&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">varH</span> <span class="o">=</span> <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;varH&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">I_sk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Ns</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>
    <span class="n">J_sjk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Ns</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>
    <span class="n">I_sk</span><span class="p">[:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;I_sk&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">J_sjk</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;J_sjk&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">vp</span> <span class="o">=</span> <span class="n">vp0_fine</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">vp</span><span class="o">.</span><span class="n">set_parameters</span><span class="p">(</span><span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;theta&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">,</span> <span class="p">:])</span>

    <span class="c1">## Potentionally prune mixture components</span>
    <span class="n">pruned</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_weights</span><span class="p">:</span>
        <span class="n">already_checked</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">vp</span><span class="o">.</span><span class="n">K</span><span class="p">,),</span> <span class="kc">False</span><span class="p">)</span>

        <span class="k">while</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">((</span><span class="n">vp</span><span class="o">.</span><span class="n">w</span> <span class="o">&lt;</span> <span class="n">options</span><span class="p">[</span><span class="s2">&quot;tol_weight&quot;</span><span class="p">])</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">already_checked</span><span class="p">):</span>
            <span class="n">vp_pruned</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">vp</span><span class="p">)</span>

            <span class="c1"># Choose a random component below threshold</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span>
                <span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">w</span> <span class="o">&lt;</span> <span class="n">options</span><span class="p">[</span><span class="s2">&quot;tol_weight&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">already_checked</span>
            <span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">idx</span><span class="p">))]</span>
            <span class="n">vp_pruned</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">vp_pruned</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
            <span class="n">vp_pruned</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">vp_pruned</span><span class="o">.</span><span class="n">eta</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
            <span class="n">vp_pruned</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">vp_pruned</span><span class="o">.</span><span class="n">sigma</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
            <span class="n">vp_pruned</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">vp_pruned</span><span class="o">.</span><span class="n">mu</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">vp_pruned</span><span class="o">.</span><span class="n">K</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="n">theta_pruned</span> <span class="o">=</span> <span class="n">vp_pruned</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span>
            <span class="c1"># Recompute ELCBO</span>
            <span class="n">elbo_stats</span> <span class="o">=</span> <span class="n">_eval_full_elcbo</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span>
                <span class="n">theta_pruned</span><span class="p">,</span>
                <span class="n">vp_pruned</span><span class="p">,</span>
                <span class="n">gp</span><span class="p">,</span>
                <span class="n">elbo_stats</span><span class="p">,</span>
                <span class="n">elcbo_beta</span><span class="p">,</span>
                <span class="n">options</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">elbo_pruned</span> <span class="o">=</span> <span class="o">-</span><span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;nelbo&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">elbo_pruned_sd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;varF&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>

            <span class="c1"># Difference in ELCBO (before and after pruning)</span>
            <span class="n">delta_elcbo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span>
                <span class="p">(</span><span class="n">elbo_pruned</span> <span class="o">-</span> <span class="n">options</span><span class="p">[</span><span class="s2">&quot;elcbo_impro_weight&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">elbo_pruned_sd</span><span class="p">)</span>
                <span class="o">-</span> <span class="p">(</span><span class="n">elbo</span> <span class="o">-</span> <span class="n">options</span><span class="p">[</span><span class="s2">&quot;elcbo_impro_weight&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">elbo_sd</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="c1"># Prune component if it has neglible influence on ELCBO</span>
            <span class="n">pruning_threshold</span> <span class="o">=</span> <span class="n">options</span><span class="p">[</span><span class="s2">&quot;tol_improvement&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">options</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span>
                <span class="s2">&quot;pruning_threshold_multiplier&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;K&quot;</span><span class="p">:</span> <span class="n">K</span><span class="p">}</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">delta_elcbo</span> <span class="o">&lt;</span> <span class="n">pruning_threshold</span><span class="p">:</span>
                <span class="n">vp</span> <span class="o">=</span> <span class="n">vp_pruned</span>
                <span class="n">elbo</span> <span class="o">=</span> <span class="n">elbo_pruned</span>
                <span class="n">elbo_sd</span> <span class="o">=</span> <span class="n">elbo_pruned_sd</span>
                <span class="n">G</span> <span class="o">=</span> <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;G&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">H</span> <span class="o">=</span> <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;H&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">var_ss</span> <span class="o">=</span> <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;var_ss&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">varG</span> <span class="o">=</span> <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;varG&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">varH</span> <span class="o">=</span> <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;varH&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">pruned</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">already_checked</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">already_checked</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
                <span class="n">I_sk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">I_sk</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">J_sjk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">J_sjk</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">already_checked</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">vp</span><span class="o">.</span><span class="n">stats</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">vp</span><span class="o">.</span><span class="n">stats</span><span class="p">[</span><span class="s2">&quot;elbo&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">elbo</span>  <span class="c1"># ELBO</span>
    <span class="n">vp</span><span class="o">.</span><span class="n">stats</span><span class="p">[</span><span class="s2">&quot;elbo_sd&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">elbo_sd</span>  <span class="c1"># Error on the ELBO</span>
    <span class="n">vp</span><span class="o">.</span><span class="n">stats</span><span class="p">[</span><span class="s2">&quot;e_log_joint&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">G</span>  <span class="c1"># Expected log joint</span>
    <span class="n">vp</span><span class="o">.</span><span class="n">stats</span><span class="p">[</span><span class="s2">&quot;e_log_joint_sd&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">varG</span><span class="p">)</span>  <span class="c1"># Error on expected log joint</span>
    <span class="n">vp</span><span class="o">.</span><span class="n">stats</span><span class="p">[</span><span class="s2">&quot;entropy&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">H</span>  <span class="c1"># Entropy</span>
    <span class="n">vp</span><span class="o">.</span><span class="n">stats</span><span class="p">[</span><span class="s2">&quot;entropy_sd&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">varH</span><span class="p">)</span>  <span class="c1"># Error on the entropy</span>
    <span class="n">vp</span><span class="o">.</span><span class="n">stats</span><span class="p">[</span><span class="s2">&quot;stable&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># Unstable until proven otherwise</span>
    <span class="n">vp</span><span class="o">.</span><span class="n">stats</span><span class="p">[</span><span class="s2">&quot;I_sk&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">I_sk</span>  <span class="c1"># Expected log joint per component</span>
    <span class="n">vp</span><span class="o">.</span><span class="n">stats</span><span class="p">[</span><span class="s2">&quot;J_sjk&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">J_sjk</span>  <span class="c1"># Covariance of expected log joint</span>

    <span class="k">return</span> <span class="n">vp</span><span class="p">,</span> <span class="n">var_ss</span><span class="p">,</span> <span class="n">pruned</span></div>


<span class="k">def</span> <span class="nf">_initialize_full_elcbo</span><span class="p">(</span><span class="n">max_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">D</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">Ns</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize a dictionary for keeping track of full ELCBO output.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ==========</span>
<span class="sd">    max_idx : int</span>
<span class="sd">        Maximum number of full ELCBO evaluations.</span>
<span class="sd">    D : int</span>
<span class="sd">        The dimension.</span>
<span class="sd">    K : int</span>
<span class="sd">        Number of mixture components.</span>
<span class="sd">    Ns : int</span>
<span class="sd">        Number of samples for entropy approximation.</span>

<span class="sd">    Returns</span>
<span class="sd">    =======</span>
<span class="sd">    elbo_stats : dict</span>
<span class="sd">        A dictionary with entries for all output variables of full ELCBO.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">elbo_stats</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;nelbo&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">max_idx</span><span class="p">,),</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;G&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">max_idx</span><span class="p">,),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;H&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">max_idx</span><span class="p">,),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;varF&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">max_idx</span><span class="p">,),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;varG&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">max_idx</span><span class="p">,),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;varH&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">max_idx</span><span class="p">,),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;var_ss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">max_idx</span><span class="p">,),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;nelcbo&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">max_idx</span><span class="p">,),</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;theta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">max_idx</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;I_sk&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">max_idx</span><span class="p">,</span> <span class="n">Ns</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;J_sjk&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">max_idx</span><span class="p">,</span> <span class="n">Ns</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">elbo_stats</span>


<span class="k">def</span> <span class="nf">_eval_full_elcbo</span><span class="p">(</span>
    <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">vp</span><span class="p">:</span> <span class="n">VariationalPosterior</span><span class="p">,</span>
    <span class="n">gp</span><span class="p">:</span> <span class="n">gpr</span><span class="o">.</span><span class="n">GP</span><span class="p">,</span>
    <span class="n">elbo_stats</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
    <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">options</span><span class="p">:</span> <span class="n">Options</span><span class="p">,</span>
    <span class="n">entropy_alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluate full ELCBO and store the results in a dictionary.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ==========</span>
<span class="sd">    idx : int</span>
<span class="sd">        Index in the dictionary to which store the evaluated values.</span>
<span class="sd">    theta : np.ndarray</span>
<span class="sd">        VP parameters for which to evaluate full ELCBO.</span>
<span class="sd">    vp : VariationalPosterior</span>
<span class="sd">        The variational posterior in question.</span>
<span class="sd">    gp : GP</span>
<span class="sd">        Gaussian process from VBMC main loop.</span>
<span class="sd">    elbo_stats : dict</span>
<span class="sd">        The dictionary for storing full ELCBO stats.</span>
<span class="sd">    beta : float</span>
<span class="sd">        Confidence weight.</span>
<span class="sd">    options : Options</span>
<span class="sd">        Options from the VBMC instance we are calling from.</span>
<span class="sd">    entropy_alpha : float, defaults to 0.0</span>
<span class="sd">        (currently unused) Parameter for lower/upper deterministic entropy</span>
<span class="sd">        interpolation.</span>

<span class="sd">    Returns</span>
<span class="sd">    =======</span>
<span class="sd">    elbo_stats : dict</span>
<span class="sd">        The updated dictionary.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Number of samples per component for MC approximation of the entropy.</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">K</span>
    <span class="n">ns_ent_fine_K</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">options</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="s2">&quot;ns_ent_fine&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;K&quot;</span><span class="p">:</span> <span class="n">K</span><span class="p">})</span> <span class="o">/</span> <span class="n">K</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;skip_elbo_variance&quot;</span> <span class="ow">in</span> <span class="n">options</span> <span class="ow">and</span> <span class="n">options</span><span class="p">[</span><span class="s2">&quot;skip_elbo_variance&quot;</span><span class="p">]:</span>
        <span class="n">compute_var</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">compute_var</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">nelbo</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">varF</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">var_ss</span><span class="p">,</span> <span class="n">varG</span><span class="p">,</span> <span class="n">varH</span><span class="p">,</span> <span class="n">I_sk</span><span class="p">,</span> <span class="n">J_sjk</span> <span class="o">=</span> <span class="n">_neg_elcbo</span><span class="p">(</span>
        <span class="n">theta</span><span class="p">,</span>
        <span class="n">gp</span><span class="p">,</span>
        <span class="n">vp</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="n">ns_ent_fine_K</span><span class="p">,</span>
        <span class="kc">False</span><span class="p">,</span>
        <span class="n">compute_var</span><span class="p">,</span>
        <span class="kc">None</span><span class="p">,</span>
        <span class="n">entropy_alpha</span><span class="p">,</span>
        <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">nelcbo</span> <span class="o">=</span> <span class="n">nelbo</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">varF</span><span class="p">)</span>

    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;nelbo&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">nelbo</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;G&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">G</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;H&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">H</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;varF&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">varF</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;varG&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">varG</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;varH&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">varH</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;var_ss&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">var_ss</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;nelcbo&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">nelcbo</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;theta&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">theta</span><span class="p">)]</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;I_sk&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">]</span> <span class="o">=</span> <span class="n">I_sk</span>
    <span class="n">elbo_stats</span><span class="p">[</span><span class="s2">&quot;J_sjk&quot;</span><span class="p">][</span><span class="n">idx</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">]</span> <span class="o">=</span> <span class="n">J_sjk</span>

    <span class="k">return</span> <span class="n">elbo_stats</span>


<span class="k">def</span> <span class="nf">_vp_bound_loss</span><span class="p">(</span>
    <span class="n">vp</span><span class="p">:</span> <span class="n">VariationalPosterior</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">theta_bnd</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
    <span class="n">tol_con</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">compute_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Variational parameter loss function for soft optimization bounds.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ==========</span>
<span class="sd">    vp : VariationalPosterior</span>
<span class="sd">        The variational posterior for which we are interested in computing</span>
<span class="sd">        the loss function on.</span>
<span class="sd">    theta : np.ndarray, shape (N,)</span>
<span class="sd">        The parameters at which we want to compute the loss function.</span>
<span class="sd">    theta_bnd : dict</span>
<span class="sd">        Variational posterior soft bounds.</span>
<span class="sd">    tol_con : float, defaults to 1e-3</span>
<span class="sd">        Penalization relative scale.</span>
<span class="sd">    compute_grad : bool, defaults to True</span>
<span class="sd">        Whether to compute gradients.</span>

<span class="sd">    Returns</span>
<span class="sd">    =======</span>
<span class="sd">    L : float</span>
<span class="sd">        The value of the loss function.</span>
<span class="sd">    dL : np.ndarray, shape (N,), optional</span>
<span class="sd">        The gradient of the loss function.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_mu</span><span class="p">:</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[:</span> <span class="n">vp</span><span class="o">.</span><span class="n">D</span> <span class="o">*</span> <span class="n">vp</span><span class="o">.</span><span class="n">K</span><span class="p">]</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">D</span> <span class="o">*</span> <span class="n">vp</span><span class="o">.</span><span class="n">K</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_sigma</span><span class="p">:</span>
        <span class="n">ln_sigma</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">start_idx</span> <span class="p">:</span> <span class="n">start_idx</span> <span class="o">+</span> <span class="n">vp</span><span class="o">.</span><span class="n">K</span><span class="p">]</span>
        <span class="n">start_idx</span> <span class="o">+=</span> <span class="n">vp</span><span class="o">.</span><span class="n">K</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ln_sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">sigma</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_lambd</span><span class="p">:</span>
        <span class="n">ln_lambd</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">start_idx</span> <span class="p">:</span> <span class="n">start_idx</span> <span class="o">+</span> <span class="n">vp</span><span class="o">.</span><span class="n">D</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ln_lambd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">lambd</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_weights</span><span class="p">:</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="o">-</span><span class="n">vp</span><span class="o">.</span><span class="n">K</span> <span class="p">:]</span>

    <span class="n">ln_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ln_lambd</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ln_sigma</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">theta_ext</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_mu</span><span class="p">:</span>
        <span class="n">theta_ext</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_sigma</span> <span class="ow">or</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_lambda</span><span class="p">:</span>
        <span class="n">theta_ext</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ln_scale</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_weights</span><span class="p">:</span>
        <span class="n">theta_ext</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">eta</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
    <span class="n">theta_ext</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">theta_ext</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">compute_grad</span><span class="p">:</span>
        <span class="n">L</span><span class="p">,</span> <span class="n">dL</span> <span class="o">=</span> <span class="n">_soft_bound_loss</span><span class="p">(</span>
            <span class="n">theta_ext</span><span class="p">,</span>
            <span class="n">theta_bnd</span><span class="p">[</span><span class="s2">&quot;lb&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
            <span class="n">theta_bnd</span><span class="p">[</span><span class="s2">&quot;ub&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
            <span class="n">tol_con</span><span class="p">,</span>
            <span class="n">compute_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">dL_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
        <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_mu</span><span class="p">:</span>
            <span class="n">dL_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">dL_new</span><span class="p">,</span> <span class="n">dL</span><span class="p">[</span><span class="mi">0</span> <span class="p">:</span> <span class="n">vp</span><span class="o">.</span><span class="n">D</span> <span class="o">*</span> <span class="n">vp</span><span class="o">.</span><span class="n">K</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">()))</span>
            <span class="n">start_idx</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">D</span> <span class="o">*</span> <span class="n">vp</span><span class="o">.</span><span class="n">K</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">start_idx</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_sigma</span> <span class="ow">or</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_lambda</span><span class="p">:</span>
            <span class="n">dlnscale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">dL</span><span class="p">[</span><span class="n">start_idx</span> <span class="p">:</span> <span class="n">start_idx</span> <span class="o">+</span> <span class="n">vp</span><span class="o">.</span><span class="n">D</span> <span class="o">*</span> <span class="n">vp</span><span class="o">.</span><span class="n">K</span><span class="p">],</span> <span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">D</span><span class="p">,</span> <span class="n">vp</span><span class="o">.</span><span class="n">K</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_sigma</span><span class="p">:</span>
                <span class="n">dL_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">dL_new</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dlnscale</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>

            <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_lambd</span><span class="p">:</span>
                <span class="n">dL_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">dL_new</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dlnscale</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>

        <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_weights</span><span class="p">:</span>
            <span class="n">dL_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">dL_new</span><span class="p">,</span> <span class="n">dL</span><span class="p">[</span><span class="o">-</span><span class="n">vp</span><span class="o">.</span><span class="n">K</span> <span class="p">:]</span><span class="o">.</span><span class="n">ravel</span><span class="p">()))</span>

        <span class="k">return</span> <span class="n">L</span><span class="p">,</span> <span class="n">dL_new</span>

    <span class="n">L</span> <span class="o">=</span> <span class="n">_soft_bound_loss</span><span class="p">(</span>
        <span class="n">theta_ext</span><span class="p">,</span>
        <span class="n">theta_bnd</span><span class="p">[</span><span class="s2">&quot;lb&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">theta_bnd</span><span class="p">[</span><span class="s2">&quot;ub&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">tol_con</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">L</span>


<span class="k">def</span> <span class="nf">_soft_bound_loss</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">slb</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">sub</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">tol_con</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">compute_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Loss function for soft bounds for function minimization.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ==========</span>
<span class="sd">    x : np.ndarray, shape (D,)</span>
<span class="sd">        Point for which we want to know the loss function value.</span>
<span class="sd">    slb : np.ndarray, shape (D,)</span>
<span class="sd">        Soft lower bounds.</span>
<span class="sd">    sub : np.ndarray, shape (D,)</span>
<span class="sd">        Soft upper bounds.</span>
<span class="sd">    tol_con : float, defaults to 1e-3</span>
<span class="sd">        Penalization relative scale.</span>
<span class="sd">    compute_grad : bool, defaults to False</span>
<span class="sd">        Whether to compute gradients.</span>

<span class="sd">    Returns</span>
<span class="sd">    =======</span>
<span class="sd">    y : float</span>
<span class="sd">        The value of the loss function.</span>
<span class="sd">    dy : np.ndarray, shape (D,), optional</span>
<span class="sd">        The gradient of the loss function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ell</span> <span class="o">=</span> <span class="p">(</span><span class="n">sub</span> <span class="o">-</span> <span class="n">slb</span><span class="p">)</span> <span class="o">*</span> <span class="n">tol_con</span>
    <span class="n">y</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">dy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">slb</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">idx</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(((</span><span class="n">slb</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">/</span> <span class="n">ell</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">compute_grad</span><span class="p">:</span>
            <span class="n">dy</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">slb</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">/</span> <span class="n">ell</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="n">sub</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">idx</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(((</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">sub</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">/</span> <span class="n">ell</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">compute_grad</span><span class="p">:</span>
            <span class="n">dy</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">sub</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">/</span> <span class="n">ell</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="k">if</span> <span class="n">compute_grad</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">dy</span>
    <span class="k">return</span> <span class="n">y</span>


<span class="k">def</span> <span class="nf">_sieve</span><span class="p">(</span>
    <span class="n">options</span><span class="p">:</span> <span class="n">Options</span><span class="p">,</span>
    <span class="n">optim_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
    <span class="n">vp</span><span class="p">:</span> <span class="n">VariationalPosterior</span><span class="p">,</span>
    <span class="n">gp</span><span class="p">:</span> <span class="n">gpr</span><span class="o">.</span><span class="n">GP</span><span class="p">,</span>
    <span class="n">init_N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">best_N</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">K</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Preliminary &#39;sieve&#39; method for fitting variational posterior.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ==========</span>
<span class="sd">    options : Options</span>
<span class="sd">        Options from the VBMC instance we are calling this from.</span>
<span class="sd">    optim_state : dict</span>
<span class="sd">        Optimization state from the VBMC instance we are calling this from.</span>
<span class="sd">    vp : VariationalPosterior</span>
<span class="sd">        The variational posterior to use as a basis for new candidates.</span>
<span class="sd">    gp : GP</span>
<span class="sd">        Current GP from optimization.</span>
<span class="sd">    init_N : int, optional</span>
<span class="sd">        Number of initial starting points.</span>
<span class="sd">    best_N : int, defaults to 1</span>
<span class="sd">        Specifies the design pattern for new starting parameters. ``best_N==1``</span>
<span class="sd">        means use the old variational parameters as a starting point for new</span>
<span class="sd">        candidate VP&#39;s. Any other value will use an even mix of:</span>

<span class="sd">        - the old variational parameters,</span>
<span class="sd">        - the highest posterior density training points, and</span>
<span class="sd">        - random starting points</span>

<span class="sd">        for new candidate VP&#39;s.</span>
<span class="sd">    K : int, optional</span>
<span class="sd">        Number of mixture components. If not given defaults to the number</span>
<span class="sd">        of mixture components the given VP has.</span>

<span class="sd">    Returns</span>
<span class="sd">    =======</span>
<span class="sd">    vp0_vec : np.ndarray, shape (init_N,)</span>
<span class="sd">        Vector of candidate variational posteriors.</span>
<span class="sd">    vp0_type : np.ndarray, shape (init_N,)</span>
<span class="sd">        Vector of types of candidate variational posteriors.</span>
<span class="sd">    elcbo_beta : float</span>
<span class="sd">        Confidence weight.</span>
<span class="sd">    compute_var : bool</span>
<span class="sd">        Whether to compute variance in later optimization.</span>
<span class="sd">    ns_ent_K : int</span>
<span class="sd">        Number of samples per component for MC approximation of the entropy.</span>
<span class="sd">    ns_ent_K_fast : int</span>
<span class="sd">        Number of samples per component for preliminary MC approximation of</span>
<span class="sd">        the entropy.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">K</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">K</span>

    <span class="c1"># Missing port: assign default values to optim_state (since</span>
    <span class="c1">#               this doesn&#39;t seem to be necessary)</span>

    <span class="c1">## Set up optimization variables and options.</span>

    <span class="c1"># Number of initial starting points</span>
    <span class="k">if</span> <span class="n">init_N</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">init_N</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">options</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="s2">&quot;ns_elbo&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;K&quot;</span><span class="p">:</span> <span class="n">K</span><span class="p">}))</span>
    <span class="n">nelcbo_fill</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">init_N</span><span class="p">,))</span>

    <span class="c1"># Number of samples per component for MC approximation of the entropy.</span>
    <span class="n">ns_ent_K</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">options</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="s2">&quot;ns_ent&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;K&quot;</span><span class="p">:</span> <span class="n">K</span><span class="p">})</span> <span class="o">/</span> <span class="n">K</span><span class="p">)</span>

    <span class="c1"># Number of samples per component for preliminary MC approximation</span>
    <span class="c1"># of the entropy.</span>
    <span class="n">ns_ent_K_fast</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">options</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="s2">&quot;ns_ent_fast&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;K&quot;</span><span class="p">:</span> <span class="n">K</span><span class="p">})</span> <span class="o">/</span> <span class="n">K</span><span class="p">)</span>

    <span class="c1"># Deterministic entropy if entropy switch is on or only one component</span>
    <span class="k">if</span> <span class="n">optim_state</span><span class="p">[</span><span class="s2">&quot;entropy_switch&quot;</span><span class="p">]</span> <span class="ow">or</span> <span class="n">K</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">ns_ent_K</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">ns_ent_K_fast</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Confidence weight</span>
    <span class="c1"># Missing port: elcboweight does not exist</span>
    <span class="c1"># elcbo_beta = self._eval_option(self.options[&quot;elcboweight&quot;],</span>
    <span class="c1">#                                self.optim_state[&quot;n_eff&quot;])</span>
    <span class="n">elcbo_beta</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">compute_var</span> <span class="o">=</span> <span class="n">elcbo_beta</span> <span class="o">!=</span> <span class="mi">0</span>

    <span class="c1"># Compute soft bounds for variational parameter optimization</span>
    <span class="n">theta_bnd</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">get_bounds</span><span class="p">(</span><span class="n">gp</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">options</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>

    <span class="c1">## Perform quick shotgun evaluation of many candidate parameters</span>

    <span class="k">if</span> <span class="n">init_N</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Get high-posterior density points</span>
        <span class="n">X_star</span><span class="p">,</span> <span class="n">y_star</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_hpd</span><span class="p">(</span><span class="n">gp</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">gp</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">options</span><span class="p">[</span><span class="s2">&quot;hpd_frac&quot;</span><span class="p">])</span>

        <span class="c1"># Generate a bunch of random candidate variational parameters.</span>
        <span class="k">if</span> <span class="n">best_N</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">vp0_vec</span><span class="p">,</span> <span class="n">vp0_type</span> <span class="o">=</span> <span class="n">_vb_init</span><span class="p">(</span><span class="n">vp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">init_N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">X_star</span><span class="p">,</span> <span class="n">y_star</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Fix random seed here if trying to reproduce MATLAB numbers</span>
            <span class="n">vp0_vec1</span><span class="p">,</span> <span class="n">vp0_type1</span> <span class="o">=</span> <span class="n">_vb_init</span><span class="p">(</span>
                <span class="n">vp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">init_N</span> <span class="o">/</span> <span class="mi">3</span><span class="p">),</span> <span class="n">K</span><span class="p">,</span> <span class="n">X_star</span><span class="p">,</span> <span class="n">y_star</span>
            <span class="p">)</span>
            <span class="n">vp0_vec2</span><span class="p">,</span> <span class="n">vp0_type2</span> <span class="o">=</span> <span class="n">_vb_init</span><span class="p">(</span>
                <span class="n">vp</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">init_N</span> <span class="o">/</span> <span class="mi">3</span><span class="p">),</span> <span class="n">K</span><span class="p">,</span> <span class="n">X_star</span><span class="p">,</span> <span class="n">y_star</span>
            <span class="p">)</span>
            <span class="n">vp0_vec3</span><span class="p">,</span> <span class="n">vp0_type3</span> <span class="o">=</span> <span class="n">_vb_init</span><span class="p">(</span>
                <span class="n">vp</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">init_N</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">init_N</span> <span class="o">/</span> <span class="mi">3</span><span class="p">),</span> <span class="n">K</span><span class="p">,</span> <span class="n">X_star</span><span class="p">,</span> <span class="n">y_star</span>
            <span class="p">)</span>
            <span class="n">vp0_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">vp0_vec1</span><span class="p">,</span> <span class="n">vp0_vec2</span><span class="p">,</span> <span class="n">vp0_vec3</span><span class="p">])</span>
            <span class="n">vp0_type</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">vp0_type1</span><span class="p">,</span> <span class="n">vp0_type2</span><span class="p">,</span> <span class="n">vp0_type3</span><span class="p">])</span>

        <span class="c1"># in MATLAB the vp_repo is used here</span>

        <span class="c1"># Quickly estimate ELCBO at each candidate variational posterior.</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">vp0</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vp0_vec</span><span class="p">):</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="n">vp0</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span>
            <span class="n">nelbo_tmp</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">varF_tmp</span> <span class="o">=</span> <span class="n">_neg_elcbo</span><span class="p">(</span>
                <span class="n">theta</span><span class="p">,</span>
                <span class="n">gp</span><span class="p">,</span>
                <span class="n">vp0</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>
                <span class="n">ns_ent_K_fast</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>
                <span class="n">compute_var</span><span class="p">,</span>
                <span class="n">theta_bnd</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">nelcbo_fill</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">nelbo_tmp</span> <span class="o">+</span> <span class="n">elcbo_beta</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">varF_tmp</span><span class="p">)</span>

        <span class="c1"># Sort by negative ELCBO</span>
        <span class="n">order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">nelcbo_fill</span><span class="p">)</span>
        <span class="n">vp0_vec</span> <span class="o">=</span> <span class="n">vp0_vec</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>
        <span class="n">vp0_type</span> <span class="o">=</span> <span class="n">vp0_type</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="n">vp0_vec</span><span class="p">,</span>
            <span class="n">vp0_type</span><span class="p">,</span>
            <span class="n">elcbo_beta</span><span class="p">,</span>
            <span class="n">compute_var</span><span class="p">,</span>
            <span class="n">ns_ent_K</span><span class="p">,</span>
            <span class="n">ns_ent_K_fast</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">vp</span><span class="p">),</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="n">elcbo_beta</span><span class="p">,</span>
        <span class="n">compute_var</span><span class="p">,</span>
        <span class="n">ns_ent_K</span><span class="p">,</span>
        <span class="n">ns_ent_K_fast</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_vb_init</span><span class="p">(</span>
    <span class="n">vp</span><span class="p">:</span> <span class="n">VariationalPosterior</span><span class="p">,</span>
    <span class="n">vb_type</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">opts_N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">K_new</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">X_star</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">y_star</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate array of random starting parameters for variational posterior.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ==========</span>
<span class="sd">    vp : VariationalPosterior</span>
<span class="sd">        Variational posterior to use as base.</span>
<span class="sd">    vb_type : {1, 2, 3}</span>
<span class="sd">        Type of method to create new starting parameters. Here 1 means</span>
<span class="sd">        starting from old variational parameters, 2 means starting from</span>
<span class="sd">        highest-posterior density training points, and 3 means starting</span>
<span class="sd">        from random provided training points.</span>
<span class="sd">    opts_N : int</span>
<span class="sd">        Number of random starting parameters.</span>
<span class="sd">    K_new : int</span>
<span class="sd">        New number of mixture components.</span>
<span class="sd">    X_star : np.ndarray, shape (N, D)</span>
<span class="sd">        Training inputs, usually HPD regions.</span>
<span class="sd">    y_star : np.ndarray, shape (N, 1)</span>
<span class="sd">        Training targets, usually HPD regions.</span>

<span class="sd">    Returns</span>
<span class="sd">    =======</span>
<span class="sd">    vp0_vec : np.ndarray, shape (opts_N, )</span>
<span class="sd">        The array of random starting parameters.</span>
<span class="sd">    type_vec : np.ndarray, shape (opts_N, )</span>
<span class="sd">        The array of type of each random starting parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">D</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">D</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">K</span>
    <span class="n">N_star</span> <span class="o">=</span> <span class="n">X_star</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">type_vec</span> <span class="o">=</span> <span class="n">vb_type</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">opts_N</span><span class="p">))</span>
    <span class="n">lambd0</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">lambd</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">mu0</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">w0</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">w</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">vb_type</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Start from old variational parameters</span>
        <span class="n">sigma0</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">sigma</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">vb_type</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># Start from highest-posterior density training points</span>
        <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_mu</span><span class="p">:</span>
            <span class="n">order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">y_star</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">idx_order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
                <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">K_new</span><span class="p">,</span> <span class="n">N_star</span><span class="p">)),</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">K_new</span> <span class="o">/</span> <span class="n">N_star</span><span class="p">),)</span>
            <span class="p">)</span>
            <span class="n">mu0</span> <span class="o">=</span> <span class="n">X_star</span><span class="p">[</span><span class="n">order</span><span class="p">[</span><span class="n">idx_order</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">K_new</span><span class="p">]],</span> <span class="p">:]</span><span class="o">.</span><span class="n">T</span>
        <span class="k">if</span> <span class="n">K</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">mu0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X_star</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sigma0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">V</span> <span class="o">/</span> <span class="n">lambd0</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">K_new</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
            <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K_new</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Start from random provided training points.</span>
        <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_mu</span><span class="p">:</span>
            <span class="n">mu0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">D</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>
        <span class="n">sigma0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>

    <span class="n">vp0_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">opts_N</span><span class="p">):</span>
        <span class="n">add_jitter</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">mu0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">lambd</span> <span class="o">=</span> <span class="n">lambd0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_weights</span><span class="p">:</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">w0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">vb_type</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Start from old variational parameters</span>

            <span class="c1"># Copy previous parameters verbatim.</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">add_jitter</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="k">if</span> <span class="n">K_new</span> <span class="o">&gt;</span> <span class="n">vp</span><span class="o">.</span><span class="n">K</span><span class="p">:</span>
                <span class="c1"># Spawn a new component near an existing one</span>
                <span class="k">for</span> <span class="n">i_new</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">K_new</span><span class="p">):</span>
                    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
                    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">mu</span><span class="p">,</span> <span class="n">mu</span><span class="p">[:,</span> <span class="n">idx</span> <span class="p">:</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
                    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">sigma</span><span class="p">,</span> <span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="n">idx</span> <span class="p">:</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
                    <span class="n">mu</span><span class="p">[:,</span> <span class="n">i_new</span> <span class="p">:</span> <span class="n">i_new</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="mf">0.5</span> <span class="o">*</span> <span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i_new</span><span class="p">]</span> <span class="o">*</span> <span class="n">lambd</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="p">)</span>

                    <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_sigma</span><span class="p">:</span>
                        <span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i_new</span><span class="p">]</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">())</span>

                    <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_weights</span><span class="p">:</span>
                        <span class="n">xi</span> <span class="o">=</span> <span class="mf">0.25</span> <span class="o">+</span> <span class="mf">0.25</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
                        <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">w</span><span class="p">,</span> <span class="n">xi</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="n">idx</span> <span class="p">:</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
                        <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">*=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">xi</span>
        <span class="k">elif</span> <span class="n">vb_type</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># Start from highest-posterior density training points</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">add_jitter</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_lambd</span><span class="p">:</span>
                <span class="n">lambd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_star</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                <span class="n">lambd</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">D</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lambd</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_weights</span><span class="p">:</span>
                <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">K_new</span><span class="p">))</span> <span class="o">/</span> <span class="n">K_new</span>
        <span class="k">elif</span> <span class="n">vb_type</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="c1"># Start from random provided training points</span>
            <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_mu</span><span class="p">:</span>
                <span class="n">order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">N_star</span><span class="p">)</span>
                <span class="n">idx_order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
                    <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">K_new</span><span class="p">,</span> <span class="n">N_star</span><span class="p">)),</span>
                    <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">K_new</span> <span class="o">/</span> <span class="n">N_star</span><span class="p">),),</span>
                <span class="p">)</span>
                <span class="n">mu</span> <span class="o">=</span> <span class="n">X_star</span><span class="p">[</span><span class="n">order</span><span class="p">[</span><span class="n">idx_order</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">K_new</span><span class="p">]],</span> <span class="p">:]</span><span class="o">.</span><span class="n">T</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mu</span> <span class="o">=</span> <span class="n">mu0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_sigma</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">K</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X_star</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">V</span><span class="p">)</span> <span class="o">/</span> <span class="n">K_new</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
                    <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K_new</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_lambd</span><span class="p">:</span>
                <span class="n">lambd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_star</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                <span class="n">lambd</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">D</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lambd</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_weights</span><span class="p">:</span>
                <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">K_new</span><span class="p">))</span> <span class="o">/</span> <span class="n">K_new</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Unknown type for initialization of variational posteriors.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">add_jitter</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_mu</span><span class="p">:</span>
                <span class="c1"># When reproducing MATLAB numbers we need to do Fortran order</span>
                <span class="c1"># here, adding .T works with square shape.</span>
                <span class="n">mu</span> <span class="o">+=</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">lambd</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_sigma</span><span class="p">:</span>
                <span class="n">sigma</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K_new</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_lambd</span><span class="p">:</span>
                <span class="n">lambd</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_weights</span><span class="p">:</span>
                <span class="n">w</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K_new</span><span class="p">))</span>
                <span class="n">w</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

        <span class="n">new_vp</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">vp</span><span class="p">)</span>
        <span class="n">new_vp</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">K_new</span>

        <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_weights</span><span class="p">:</span>
            <span class="n">new_vp</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">w</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_vp</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">K_new</span><span class="p">))</span> <span class="o">/</span> <span class="n">K_new</span>
        <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_mu</span><span class="p">:</span>
            <span class="n">new_vp</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_vp</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">new_vp</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
        <span class="n">new_vp</span><span class="o">.</span><span class="n">lambd</span> <span class="o">=</span> <span class="n">lambd</span>
        <span class="c1"># TODO: just set to None?</span>
        <span class="n">new_vp</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">K_new</span><span class="p">))</span> <span class="o">/</span> <span class="n">K_new</span>
        <span class="n">new_vp</span><span class="o">.</span><span class="n">bounds</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">new_vp</span><span class="o">.</span><span class="n">stats</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">vp0_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_vp</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vp0_list</span><span class="p">),</span> <span class="n">type_vec</span>


<span class="k">def</span> <span class="nf">_neg_elcbo</span><span class="p">(</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">gp</span><span class="p">:</span> <span class="n">gpr</span><span class="o">.</span><span class="n">GP</span><span class="p">,</span>
    <span class="n">vp</span><span class="p">:</span> <span class="n">VariationalPosterior</span><span class="p">,</span>
    <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">Ns</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">compute_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">compute_var</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">theta_bnd</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">_entropy_alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">separate_K</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Negative evidence lower confidence bound objective.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ==========</span>
<span class="sd">    theta : np.ndarray</span>
<span class="sd">        Vector of variational parameters at which to evaluate NELCBO.</span>
<span class="sd">        Note that these should be transformed parameters.</span>
<span class="sd">    gp : GP</span>
<span class="sd">        Gaussian process from optimization</span>
<span class="sd">    vp : VariationalPosterior</span>
<span class="sd">        Variational posterior for which to evaluate NELCBO.</span>
<span class="sd">    beta : float, defaults to 0.0</span>
<span class="sd">        Confidence weight.</span>
<span class="sd">    Ns : int, defaults to 0</span>
<span class="sd">        Number of samples for entropy.</span>
<span class="sd">    compute_grad : bool, defaults to True</span>
<span class="sd">        Whether to compute gradient.</span>
<span class="sd">    compute_var : bool, optional</span>
<span class="sd">        Whether to compute variance. If not given this is</span>
<span class="sd">        determined automatically.</span>
<span class="sd">    theta_bnd : dict, optional</span>
<span class="sd">        Soft bounds for theta.</span>
<span class="sd">    entropy_alpha : float, defaults to 0.0</span>
<span class="sd">        (currently unused) Parameter for lower/upper deterministic entropy</span>
<span class="sd">        interpolation.</span>
<span class="sd">    separate_K : bool, defaults to False</span>
<span class="sd">        Whether to return expected log joint per component.</span>

<span class="sd">    Returns</span>
<span class="sd">    =======</span>
<span class="sd">    F : float</span>
<span class="sd">        Negative evidence lower confidence bound objective.</span>
<span class="sd">    dF : np.ndarray</span>
<span class="sd">        Gradient of NELCBO.</span>
<span class="sd">    G : object</span>
<span class="sd">        The expected variational log joint probability.</span>
<span class="sd">    H : float</span>
<span class="sd">        Entropy term.</span>
<span class="sd">    varF : float</span>
<span class="sd">        Variance of NELCBO.</span>
<span class="sd">    dH : np.ndarray</span>
<span class="sd">        Gradient of entropy term.</span>
<span class="sd">    varG_ss :</span>
<span class="sd">        Variance of the expected variational log joint, for each GP</span>
<span class="sd">        hyperparameter sample.</span>
<span class="sd">    varG :</span>
<span class="sd">        Variance of the expected variational log joint</span>
<span class="sd">        probability.</span>
<span class="sd">    varH : float</span>
<span class="sd">        Variance of entropy term.</span>
<span class="sd">    I_sk : np.ndarray</span>
<span class="sd">        The contribution to ``G`` per GP hyperparameter sample and per VP</span>
<span class="sd">        component.</span>
<span class="sd">    J_sjk : np.ndarray</span>
<span class="sd">        The contribution to ``varG`` per GP hyperparameter sample and per pair</span>
<span class="sd">        of VP components.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">beta</span><span class="p">):</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">compute_var</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">compute_var</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">!=</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">compute_grad</span> <span class="ow">and</span> <span class="n">beta</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">compute_var</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Computation of the gradient of ELBO with full variance not &quot;</span>
            <span class="s2">&quot;supported&quot;</span>
        <span class="p">)</span>

    <span class="n">K</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">K</span>

    <span class="c1"># Average over multiple GP hyperparameters if provided</span>
    <span class="n">avg_flag</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1"># Variational parameters are transformed</span>
    <span class="n">jacobian_flag</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># Reformat variational parameters from theta.</span>
    <span class="n">vp</span><span class="o">.</span><span class="n">set_parameters</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_weights</span><span class="p">:</span>
        <span class="n">vp</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="o">-</span><span class="n">K</span><span class="p">:]</span>
        <span class="n">vp</span><span class="o">.</span><span class="n">eta</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">eta</span><span class="p">)</span>
        <span class="n">vp</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">eta</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="c1"># Doing the above is more numerically robust than</span>
        <span class="c1"># below, but it might cause slightly different results</span>
        <span class="c1"># to MATLAB in some cases.</span>
        <span class="c1"># vp.eta = np.reshape(theta[-K:], (1, -1))</span>

    <span class="c1"># Which gradients should be computed, if any?</span>
    <span class="k">if</span> <span class="n">compute_grad</span><span class="p">:</span>
        <span class="n">grad_flags</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">vp</span><span class="o">.</span><span class="n">optimize_mu</span><span class="p">,</span>
            <span class="n">vp</span><span class="o">.</span><span class="n">optimize_sigma</span><span class="p">,</span>
            <span class="n">vp</span><span class="o">.</span><span class="n">optimize_lambd</span><span class="p">,</span>
            <span class="n">vp</span><span class="o">.</span><span class="n">optimize_weights</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">grad_flags</span> <span class="o">=</span> <span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Only weight optimization?</span>
    <span class="c1"># Not currently used, since it is only a speed optimization.</span>
    <span class="c1"># onlyweights_flag = (</span>
    <span class="c1">#     vp.optimize_weights</span>
    <span class="c1">#     and not vp.optimize_mu</span>
    <span class="c1">#     and not vp.optimize_sigma</span>
    <span class="c1">#     and not vp.optimize_lambd</span>
    <span class="c1"># )</span>

    <span class="c1"># Missing port: block below does not have branches for only weight</span>
    <span class="c1">#               optimization</span>
    <span class="k">if</span> <span class="n">separate_K</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">compute_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Computing the gradient of variational parameters and &quot;</span>
                <span class="s2">&quot;requesting per-component results at the same time.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">compute_var</span><span class="p">:</span>
            <span class="n">G</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">varG</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">varG_ss</span><span class="p">,</span> <span class="n">I_sk</span><span class="p">,</span> <span class="n">J_sjk</span> <span class="o">=</span> <span class="n">_gp_log_joint</span><span class="p">(</span>
                <span class="n">vp</span><span class="p">,</span>
                <span class="n">gp</span><span class="p">,</span>
                <span class="n">grad_flags</span><span class="p">,</span>
                <span class="n">avg_flag</span><span class="p">,</span>
                <span class="n">jacobian_flag</span><span class="p">,</span>
                <span class="n">compute_var</span><span class="p">,</span>
                <span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">G</span><span class="p">,</span> <span class="n">dG</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">I_sk</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_gp_log_joint</span><span class="p">(</span>
                <span class="n">vp</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">grad_flags</span><span class="p">,</span> <span class="n">avg_flag</span><span class="p">,</span> <span class="n">jacobian_flag</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span>
            <span class="p">)</span>
            <span class="n">varG</span> <span class="o">=</span> <span class="n">varG_ss</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">J_sjk</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">compute_var</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">compute_grad</span><span class="p">:</span>
                <span class="n">G</span><span class="p">,</span> <span class="n">dG</span><span class="p">,</span> <span class="n">varG</span><span class="p">,</span> <span class="n">dvarG</span><span class="p">,</span> <span class="n">varG_ss</span> <span class="o">=</span> <span class="n">_gp_log_joint</span><span class="p">(</span>
                    <span class="n">vp</span><span class="p">,</span>
                    <span class="n">gp</span><span class="p">,</span>
                    <span class="n">grad_flags</span><span class="p">,</span>
                    <span class="n">avg_flag</span><span class="p">,</span>
                    <span class="n">jacobian_flag</span><span class="p">,</span>
                    <span class="n">compute_var</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">G</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">varG</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">varG_ss</span> <span class="o">=</span> <span class="n">_gp_log_joint</span><span class="p">(</span>
                    <span class="n">vp</span><span class="p">,</span>
                    <span class="n">gp</span><span class="p">,</span>
                    <span class="n">grad_flags</span><span class="p">,</span>
                    <span class="n">avg_flag</span><span class="p">,</span>
                    <span class="n">jacobian_flag</span><span class="p">,</span>
                    <span class="n">compute_var</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">G</span><span class="p">,</span> <span class="n">dG</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_gp_log_joint</span><span class="p">(</span>
                <span class="n">vp</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">grad_flags</span><span class="p">,</span> <span class="n">avg_flag</span><span class="p">,</span> <span class="n">jacobian_flag</span><span class="p">,</span> <span class="mi">0</span>
            <span class="p">)</span>
            <span class="n">varG</span> <span class="o">=</span> <span class="n">varG_ss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Entropy term</span>
    <span class="k">if</span> <span class="n">Ns</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Monte carlo approximation</span>
        <span class="n">H</span><span class="p">,</span> <span class="n">dH</span> <span class="o">=</span> <span class="n">entmc_vbmc</span><span class="p">(</span><span class="n">vp</span><span class="p">,</span> <span class="n">Ns</span><span class="p">,</span> <span class="n">grad_flags</span><span class="p">,</span> <span class="n">jacobian_flag</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Deterministic approximation via lower bound on the entropy</span>
        <span class="n">H</span><span class="p">,</span> <span class="n">dH</span> <span class="o">=</span> <span class="n">entlb_vbmc</span><span class="p">(</span><span class="n">vp</span><span class="p">,</span> <span class="n">grad_flags</span><span class="p">,</span> <span class="n">jacobian_flag</span><span class="p">)</span>

    <span class="c1"># Negative ELBO and its gradient</span>
    <span class="n">F</span> <span class="o">=</span> <span class="o">-</span><span class="n">G</span> <span class="o">-</span> <span class="n">H</span>
    <span class="k">if</span> <span class="n">compute_grad</span><span class="p">:</span>
        <span class="n">dF</span> <span class="o">=</span> <span class="o">-</span><span class="n">dG</span> <span class="o">-</span> <span class="n">dH</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dF</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">dH</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># For the moment use zero variance for entropy</span>
    <span class="n">varH</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">compute_var</span><span class="p">:</span>
        <span class="n">varF</span> <span class="o">=</span> <span class="n">varG</span> <span class="o">+</span> <span class="n">varH</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">varF</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Negative ELCBO (add confidence bound)</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">F</span> <span class="o">+=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">varF</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">compute_grad</span><span class="p">:</span>
            <span class="n">dF</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">dvarG</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">varF</span><span class="p">)</span>

    <span class="c1"># Additional loss for variational parameter bound violation (soft bounds)</span>
    <span class="c1"># and for weight size (if optimizing mixture weights)</span>
    <span class="c1"># Only done when optimizing the variational parameters, but not when</span>
    <span class="c1"># computing the EL(C)BO at each iteration.</span>
    <span class="k">if</span> <span class="n">theta_bnd</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">compute_grad</span><span class="p">:</span>
            <span class="n">L</span><span class="p">,</span> <span class="n">dL</span> <span class="o">=</span> <span class="n">_vp_bound_loss</span><span class="p">(</span>
                <span class="n">vp</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">theta_bnd</span><span class="p">,</span> <span class="n">tol_con</span><span class="o">=</span><span class="n">theta_bnd</span><span class="p">[</span><span class="s2">&quot;tol_con&quot;</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">dF</span> <span class="o">+=</span> <span class="n">dL</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">L</span> <span class="o">=</span> <span class="n">_vp_bound_loss</span><span class="p">(</span>
                <span class="n">vp</span><span class="p">,</span>
                <span class="n">theta</span><span class="p">,</span>
                <span class="n">theta_bnd</span><span class="p">,</span>
                <span class="n">tol_con</span><span class="o">=</span><span class="n">theta_bnd</span><span class="p">[</span><span class="s2">&quot;tol_con&quot;</span><span class="p">],</span>
                <span class="n">compute_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">F</span> <span class="o">+=</span> <span class="n">L</span>

        <span class="c1">#  Penalty to reduce weight size.</span>
        <span class="k">if</span> <span class="n">vp</span><span class="o">.</span><span class="n">optimize_weights</span><span class="p">:</span>
            <span class="n">thresh</span> <span class="o">=</span> <span class="n">theta_bnd</span><span class="p">[</span><span class="s2">&quot;weight_threshold&quot;</span><span class="p">]</span>
            <span class="n">L</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">w</span> <span class="o">*</span> <span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">w</span> <span class="o">&lt;</span> <span class="n">thresh</span><span class="p">)</span> <span class="o">+</span> <span class="n">thresh</span> <span class="o">*</span> <span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">w</span> <span class="o">&gt;=</span> <span class="n">thresh</span><span class="p">))</span>
                <span class="o">*</span> <span class="n">theta_bnd</span><span class="p">[</span><span class="s2">&quot;weight_penalty&quot;</span><span class="p">]</span>
            <span class="p">)</span>

            <span class="n">F</span> <span class="o">+=</span> <span class="n">L</span>
            <span class="k">if</span> <span class="n">compute_grad</span><span class="p">:</span>
                <span class="n">w_grad</span> <span class="o">=</span> <span class="n">theta_bnd</span><span class="p">[</span><span class="s2">&quot;weight_penalty&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">w</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">thresh</span><span class="p">)</span>
                <span class="n">eta_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">eta</span><span class="p">))</span>
                <span class="n">J_w</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">eta</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">eta</span><span class="p">)</span> <span class="o">/</span> <span class="n">eta_sum</span><span class="o">**</span><span class="mi">2</span>
                <span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">eta</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span> <span class="o">/</span> <span class="n">eta_sum</span><span class="p">)</span>
                <span class="n">w_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">J_w</span><span class="p">,</span> <span class="n">w_grad</span><span class="p">)</span>
                <span class="n">dL</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dF</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="n">dL</span><span class="p">[</span><span class="o">-</span><span class="n">vp</span><span class="o">.</span><span class="n">K</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">w_grad</span>
                <span class="n">dF</span> <span class="o">+=</span> <span class="n">dL</span>

    <span class="c1"># Missing port: way to return stuff here is not that good,</span>
    <span class="c1">#               though it works currently.</span>
    <span class="k">if</span> <span class="n">separate_K</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">,</span> <span class="n">dF</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">varF</span><span class="p">,</span> <span class="n">dH</span><span class="p">,</span> <span class="n">varG_ss</span><span class="p">,</span> <span class="n">varG</span><span class="p">,</span> <span class="n">varH</span><span class="p">,</span> <span class="n">I_sk</span><span class="p">,</span> <span class="n">J_sjk</span>
    <span class="k">return</span> <span class="n">F</span><span class="p">,</span> <span class="n">dF</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">varF</span>


<span class="k">def</span> <span class="nf">_gp_log_joint</span><span class="p">(</span>
    <span class="n">vp</span><span class="p">:</span> <span class="n">VariationalPosterior</span><span class="p">,</span>
    <span class="n">gp</span><span class="p">:</span> <span class="n">gpr</span><span class="o">.</span><span class="n">GP</span><span class="p">,</span>
    <span class="n">grad_flags</span><span class="p">,</span>
    <span class="n">avg_flag</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">jacobian_flag</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">compute_var</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">separate_K</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Expected variational log joint probability via GP approximation.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ==========</span>
<span class="sd">    vp : VariationalPosterior</span>
<span class="sd">        Variational posterior.</span>
<span class="sd">    gp : GP</span>
<span class="sd">        Gaussian process from optimization.</span>
<span class="sd">    grad_flags : object</span>
<span class="sd">        Flags on which gradients to compute. If a boolean then this</span>
<span class="sd">        sets all flags to the boolean value, and if a 4-tuple then</span>
<span class="sd">        each entry specifies which gradients to compute, in order</span>
<span class="sd">        mu, sigma, lambd, w.</span>
<span class="sd">    avg_flag : bool, defaults to True</span>
<span class="sd">        Whether to average over multiple GP hyperparameters if provided.</span>
<span class="sd">    jacobian_flag : bool, defaults to True</span>
<span class="sd">        Whether variational parameters are transformed.</span>
<span class="sd">    compute_var : bool, defaults to False</span>
<span class="sd">        Whether to compute variance.</span>
<span class="sd">    separate_K : bool, defaults to False</span>
<span class="sd">        Whether to return expected log joint per component.</span>

<span class="sd">    Returns</span>
<span class="sd">    =======</span>
<span class="sd">    G : object</span>
<span class="sd">        The expected variational log joint probability.</span>
<span class="sd">    dG : np.ndarray</span>
<span class="sd">        The gradient.</span>
<span class="sd">    varG : np.ndarray, optional</span>
<span class="sd">        The variance.</span>
<span class="sd">    dvarG : np.ndarray, optional</span>
<span class="sd">        The gradient of the variance.</span>
<span class="sd">    var_ss : float</span>
<span class="sd">        Variance for each GP hyperparameter sample.</span>
<span class="sd">    I_sk : np.ndarray</span>
<span class="sd">        The contribution to ``G`` per GP hyperparameter sample and per VP</span>
<span class="sd">        component.</span>
<span class="sd">    J_sjk : np.ndarray</span>
<span class="sd">        The contribution to ``varG`` per GP hyperparameter sample and per pair</span>
<span class="sd">        of VP components.</span>

<span class="sd">    Raises</span>
<span class="sd">    ------</span>
<span class="sd">    NotImplementedError</span>
<span class="sd">        If the diagonal approximation of the gradient is requested</span>
<span class="sd">        (``compute_var == 2``) or if the gradient of the variance is requested</span>
<span class="sd">        without the diagonal approximation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">grad_flags</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">grad_flags</span><span class="p">:</span>
            <span class="n">grad_flags</span> <span class="o">=</span> <span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">grad_flags</span> <span class="o">=</span> <span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">compute_vargrad</span> <span class="o">=</span> <span class="n">compute_var</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">grad_flags</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">compute_vargrad</span> <span class="ow">and</span> <span class="n">compute_var</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Computation of gradient of log joint variance is currently &quot;</span>
            <span class="s2">&quot;available only for diagonal approximation of the variance.&quot;</span>
        <span class="p">)</span>

    <span class="n">D</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">D</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">K</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">sigma</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">lambd</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">lambd</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">w</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">w</span><span class="o">.</span><span class="n">copy</span><span class="p">()[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">Ns</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">gp</span><span class="o">.</span><span class="n">posteriors</span><span class="p">)</span>

    <span class="c1"># TODO: once we get more mean function add a check here</span>
    <span class="c1"># if all(gp.meanfun ~= [0,1,4,6,8,10,12,14,16,18,20,22])</span>
    <span class="c1">#     error(&#39;gp_log_joint:UnsupportedMeanFun&#39;, ...</span>
    <span class="c1">#     &#39;Log joint computation currently only supports zero, constant,</span>
    <span class="c1">#     negative quadratic, negative quadratic (fixed/isotropic),</span>
    <span class="c1">#     negative quadratic-only, or squared exponential mean functions.&#39;);</span>
    <span class="c1"># end</span>

    <span class="c1"># Which mean function is being used?</span>
    <span class="n">quadratic_meanfun</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="n">gp</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">gpr</span><span class="o">.</span><span class="n">mean_functions</span><span class="o">.</span><span class="n">NegativeQuadratic</span>
    <span class="p">)</span>

    <span class="n">G</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Ns</span><span class="p">,))</span>
    <span class="c1"># Check which gradients are computed</span>
    <span class="k">if</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">mu_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">D</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">Ns</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">sigma_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">Ns</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
        <span class="n">lambd_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">D</span><span class="p">,</span> <span class="n">Ns</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">3</span><span class="p">]:</span>
        <span class="n">w_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">Ns</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">compute_var</span><span class="p">:</span>
        <span class="n">varG</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Ns</span><span class="p">,))</span>
    <span class="c1"># Compute gradient of variance?</span>
    <span class="k">if</span> <span class="n">compute_vargrad</span><span class="p">:</span>
        <span class="c1"># TODO: compute vargrad is untested</span>
        <span class="k">if</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">mu_vargrad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">D</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">Ns</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">sigma_vargrad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">Ns</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
            <span class="n">lambd_vargrad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">D</span><span class="p">,</span> <span class="n">Ns</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">3</span><span class="p">]:</span>
            <span class="n">w_vargrad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">Ns</span><span class="p">))</span>

    <span class="c1"># Store contribution to the jog joint separately for each component?</span>
    <span class="k">if</span> <span class="n">separate_K</span><span class="p">:</span>
        <span class="n">I_sk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Ns</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">compute_var</span><span class="p">:</span>
            <span class="n">J_sjk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Ns</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>

    <span class="n">Xt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
        <span class="n">Xt</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mu</span><span class="p">[:,</span> <span class="n">k</span><span class="p">],</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">-</span> <span class="n">gp</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">T</span>

    <span class="c1"># Number of GP hyperparameters</span>
    <span class="n">cov_N</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">covariance</span><span class="o">.</span><span class="n">hyperparameter_count</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
    <span class="c1"># mean_N = gp.mean.hyperparameter_count(D)</span>
    <span class="n">noise_N</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">noise</span><span class="o">.</span><span class="n">hyperparameter_count</span><span class="p">()</span>

    <span class="c1"># Loop over hyperparameter samples.</span>
    <span class="c1"># Missing port: below loop does not have code related to mean functions</span>
    <span class="c1">#               we haven&#39;t implemented in gpyreg</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Ns</span><span class="p">):</span>
        <span class="n">hyp</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">posteriors</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">hyp</span>

        <span class="c1"># Extract GP hyperparameters from hyperparameter array.</span>
        <span class="n">ell</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">hyp</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">D</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">ln_sf2</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">hyp</span><span class="p">[</span><span class="n">D</span><span class="p">]</span>
        <span class="n">sum_lnell</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">hyp</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">D</span><span class="p">])</span>

        <span class="c1"># GP mean function hyperparameters</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gp</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">gpr</span><span class="o">.</span><span class="n">mean_functions</span><span class="o">.</span><span class="n">ZeroMean</span><span class="p">):</span>
            <span class="n">m0</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">m0</span> <span class="o">=</span> <span class="n">hyp</span><span class="p">[</span><span class="n">cov_N</span> <span class="o">+</span> <span class="n">noise_N</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">quadratic_meanfun</span><span class="p">:</span>
            <span class="n">xm</span> <span class="o">=</span> <span class="n">hyp</span><span class="p">[</span><span class="n">cov_N</span> <span class="o">+</span> <span class="n">noise_N</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">cov_N</span> <span class="o">+</span> <span class="n">noise_N</span> <span class="o">+</span> <span class="n">D</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
            <span class="p">)</span>
            <span class="n">omega</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">hyp</span><span class="p">[</span><span class="n">cov_N</span> <span class="o">+</span> <span class="n">noise_N</span> <span class="o">+</span> <span class="n">D</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># GP posterior parameters</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">posteriors</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">alpha</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">posteriors</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">L</span>
        <span class="n">L_chol</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">posteriors</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">L_chol</span>
        <span class="n">sn2_eff</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">gp</span><span class="o">.</span><span class="n">posteriors</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">sW</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
            <span class="n">tau_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">lambd</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">ell</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">lnnf_k</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">ln_sf2</span> <span class="o">+</span> <span class="n">sum_lnell</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tau_k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># Covariance normalization factor</span>
            <span class="n">delta_k</span> <span class="o">=</span> <span class="n">Xt</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">/</span> <span class="n">tau_k</span>
            <span class="n">z_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lnnf_k</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">delta_k</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
            <span class="n">I_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z_k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">+</span> <span class="n">m0</span>

            <span class="k">if</span> <span class="n">quadratic_meanfun</span><span class="p">:</span>
                <span class="n">nu_k</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                    <span class="mi">1</span>
                    <span class="o">/</span> <span class="n">omega</span><span class="o">**</span><span class="mi">2</span>
                    <span class="o">*</span> <span class="p">(</span>
                        <span class="n">mu</span><span class="p">[:,</span> <span class="n">k</span> <span class="p">:</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
                        <span class="o">+</span> <span class="n">sigma</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">lambd</span><span class="o">**</span><span class="mi">2</span>
                        <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">mu</span><span class="p">[:,</span> <span class="n">k</span> <span class="p">:</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">xm</span>
                        <span class="o">+</span> <span class="n">xm</span><span class="o">**</span><span class="mi">2</span>
                    <span class="p">),</span>
                    <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">I_k</span> <span class="o">+=</span> <span class="n">nu_k</span>
            <span class="n">G</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">I_k</span>

            <span class="k">if</span> <span class="n">separate_K</span><span class="p">:</span>
                <span class="n">I_sk</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">I_k</span>

            <span class="k">if</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">dz_dmu</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">delta_k</span> <span class="o">/</span> <span class="n">tau_k</span><span class="p">)</span> <span class="o">*</span> <span class="n">z_k</span>
                <span class="n">mu_grad</span><span class="p">[:,</span> <span class="n">k</span><span class="p">,</span> <span class="n">s</span> <span class="p">:</span> <span class="n">s</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dz_dmu</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">quadratic_meanfun</span><span class="p">:</span>
                    <span class="n">mu_grad</span><span class="p">[:,</span> <span class="n">k</span><span class="p">,</span> <span class="n">s</span> <span class="p">:</span> <span class="n">s</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span>
                        <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">/</span> <span class="n">omega</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">mu</span><span class="p">[:,</span> <span class="n">k</span> <span class="p">:</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">xm</span><span class="p">)</span>
                    <span class="p">)</span>

            <span class="k">if</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">dz_dsigma</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">lambd</span> <span class="o">/</span> <span class="n">tau_k</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">delta_k</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                    <span class="o">*</span> <span class="n">sigma</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span>
                    <span class="o">*</span> <span class="n">z_k</span>
                <span class="p">)</span>
                <span class="n">sigma_grad</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dz_dsigma</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">quadratic_meanfun</span><span class="p">:</span>
                    <span class="n">sigma_grad</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">s</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span>
                        <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
                        <span class="o">*</span> <span class="n">sigma</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span>
                        <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">omega</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">lambd</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                    <span class="p">)</span>

            <span class="k">if</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
                <span class="n">dz_dlambd</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="p">(</span><span class="n">sigma</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">/</span> <span class="n">tau_k</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
                    <span class="o">*</span> <span class="p">(</span><span class="n">delta_k</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="o">*</span> <span class="p">(</span><span class="n">lambd</span> <span class="o">*</span> <span class="n">z_k</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">lambd_grad</span><span class="p">[:,</span> <span class="n">s</span> <span class="p">:</span> <span class="n">s</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dz_dlambd</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">quadratic_meanfun</span><span class="p">:</span>
                    <span class="n">lambd_grad</span><span class="p">[:,</span> <span class="n">s</span> <span class="p">:</span> <span class="n">s</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span>
                        <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">sigma</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">omega</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">lambd</span>
                    <span class="p">)</span>

            <span class="k">if</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">3</span><span class="p">]:</span>
                <span class="n">w_grad</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">I_k</span>

            <span class="k">if</span> <span class="n">compute_var</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="c1"># Missing port: compute_var == 2 skipped since it is not used</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                    <span class="s2">&quot;Diagonal approximation of GP log-joint variance not implemented.&quot;</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">compute_var</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="n">tau_j</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">lambd</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">ell</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
                    <span class="n">lnnf_j</span> <span class="o">=</span> <span class="n">ln_sf2</span> <span class="o">+</span> <span class="n">sum_lnell</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tau_j</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                    <span class="n">delta_j</span> <span class="o">=</span> <span class="p">(</span><span class="n">mu</span><span class="p">[:,</span> <span class="n">j</span> <span class="p">:</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">gp</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="n">tau_j</span>
                    <span class="n">z_j</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lnnf_j</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">delta_j</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

                    <span class="n">tau_jk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
                        <span class="p">(</span><span class="n">sigma</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">sigma</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">lambd</span><span class="o">**</span><span class="mi">2</span>
                        <span class="o">+</span> <span class="n">ell</span><span class="o">**</span><span class="mi">2</span>
                    <span class="p">)</span>
                    <span class="n">lnnf_jk</span> <span class="o">=</span> <span class="n">ln_sf2</span> <span class="o">+</span> <span class="n">sum_lnell</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tau_jk</span><span class="p">))</span>
                    <span class="n">delta_jk</span> <span class="o">=</span> <span class="p">(</span><span class="n">mu</span><span class="p">[:,</span> <span class="n">j</span> <span class="p">:</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[:,</span> <span class="n">k</span> <span class="p">:</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="n">tau_jk</span>

                    <span class="n">J_jk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
                        <span class="n">lnnf_jk</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">delta_jk</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">L_chol</span><span class="p">:</span>
                        <span class="n">J_jk</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
                            <span class="n">z_k</span><span class="p">,</span>
                            <span class="n">sp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve_triangular</span><span class="p">(</span>
                                <span class="n">L</span><span class="p">,</span>
                                <span class="n">sp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve_triangular</span><span class="p">(</span>
                                    <span class="n">L</span><span class="p">,</span> <span class="n">z_j</span><span class="p">,</span> <span class="n">trans</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">check_finite</span><span class="o">=</span><span class="kc">False</span>
                                <span class="p">),</span>
                                <span class="n">trans</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                <span class="n">check_finite</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="p">)</span>
                            <span class="o">/</span> <span class="n">sn2_eff</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">J_jk</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z_k</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">z_j</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>

                    <span class="c1"># Off-diagonal elements are symmetric (count twice)</span>
                    <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="n">k</span><span class="p">:</span>
                        <span class="n">varG</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">spacing</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">J_jk</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">separate_K</span><span class="p">:</span>
                            <span class="n">J_sjk</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">J_jk</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">varG</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">J_jk</span>
                        <span class="k">if</span> <span class="n">separate_K</span><span class="p">:</span>
                            <span class="n">J_sjk</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">J_jk</span>
                            <span class="n">J_sjk</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">J_jk</span>

    <span class="c1"># Correct for numerical error</span>
    <span class="k">if</span> <span class="n">compute_var</span><span class="p">:</span>
        <span class="n">varG</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">varG</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">spacing</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">varG</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">grad_flags</span><span class="p">):</span>
        <span class="n">grad_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">mu_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mu_grad</span><span class="p">,</span> <span class="p">(</span><span class="n">D</span> <span class="o">*</span> <span class="n">K</span><span class="p">,</span> <span class="n">Ns</span><span class="p">),</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>
            <span class="n">grad_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mu_grad</span><span class="p">)</span>

        <span class="c1"># Correct for standard log reparametrization of sigma</span>
        <span class="k">if</span> <span class="n">jacobian_flag</span> <span class="ow">and</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">sigma_grad</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">grad_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sigma_grad</span><span class="p">)</span>

        <span class="c1"># Correct for standard log reparametrization of lambd</span>
        <span class="k">if</span> <span class="n">jacobian_flag</span> <span class="ow">and</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
            <span class="n">lambd_grad</span> <span class="o">*=</span> <span class="n">lambd</span>
            <span class="n">grad_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lambd_grad</span><span class="p">)</span>

        <span class="c1"># Correct for standard softmax reparametrization of w</span>
        <span class="k">if</span> <span class="n">jacobian_flag</span> <span class="ow">and</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">3</span><span class="p">]:</span>
            <span class="n">eta_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">eta</span><span class="p">))</span>
            <span class="n">J_w</span> <span class="o">=</span> <span class="p">(</span>
                <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">eta</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">eta</span><span class="p">)</span> <span class="o">/</span> <span class="n">eta_sum</span><span class="o">**</span><span class="mi">2</span>
                <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">vp</span><span class="o">.</span><span class="n">eta</span><span class="o">.</span><span class="n">ravel</span><span class="p">()))</span> <span class="o">/</span> <span class="n">eta_sum</span>
            <span class="p">)</span>
            <span class="n">w_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">J_w</span><span class="p">,</span> <span class="n">w_grad</span><span class="p">)</span>
            <span class="n">grad_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w_grad</span><span class="p">)</span>

        <span class="n">dG</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">grad_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dG</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">compute_vargrad</span><span class="p">:</span>
        <span class="c1"># TODO: compute vargrad is untested</span>
        <span class="n">vargrad_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">mu_vargrad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mu_vargrad</span><span class="p">,</span> <span class="p">(</span><span class="n">D</span> <span class="o">*</span> <span class="n">K</span><span class="p">,</span> <span class="n">Ns</span><span class="p">))</span>
            <span class="n">vargrad_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mu_vargrad</span><span class="p">)</span>

        <span class="c1"># Correct for standard log reparametrization of sigma</span>
        <span class="k">if</span> <span class="n">jacobian_flag</span> <span class="ow">and</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">sigma_vargrad</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sigma_vargrad</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">vargrad_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sigma_vargrad</span><span class="p">)</span>

        <span class="c1"># Correct for standard log reparametrization of lambd</span>
        <span class="k">if</span> <span class="n">jacobian_flag</span> <span class="ow">and</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
            <span class="n">lambd_vargrad</span> <span class="o">*=</span> <span class="n">lambd</span>
            <span class="n">vargrad_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lambd_vargrad</span><span class="p">)</span>

        <span class="c1"># Correct for standard softmax reparametrization of w</span>
        <span class="k">if</span> <span class="n">jacobian_flag</span> <span class="ow">and</span> <span class="n">grad_flags</span><span class="p">[</span><span class="mi">3</span><span class="p">]:</span>
            <span class="n">w_vargrad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">J_w</span><span class="p">,</span> <span class="n">w_vargrad</span><span class="p">)</span>
            <span class="n">vargrad_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w_vargrad</span><span class="p">)</span>

        <span class="n">dvarG</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">grad_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dvarG</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Average multiple hyperparameter samples</span>
    <span class="n">var_ss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">Ns</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">avg_flag</span><span class="p">:</span>
        <span class="n">G_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">G</span><span class="p">)</span> <span class="o">/</span> <span class="n">Ns</span>
        <span class="k">if</span> <span class="n">compute_var</span><span class="p">:</span>
            <span class="c1"># Estimated variance of the samples</span>
            <span class="n">varG_ss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">G</span> <span class="o">-</span> <span class="n">G_bar</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">Ns</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Variability due to sampling</span>
            <span class="n">var_ss</span> <span class="o">=</span> <span class="n">varG_ss</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">varG</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">varG</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">varG</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">Ns</span> <span class="o">+</span> <span class="n">varG_ss</span>
        <span class="k">if</span> <span class="n">compute_vargrad</span><span class="p">:</span>
            <span class="c1"># TODO: compute vargrad is untested</span>
            <span class="n">dvv</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">G</span> <span class="o">*</span> <span class="n">dG</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">Ns</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">G_bar</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                <span class="n">dG</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">Ns</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">dvarG</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dvarG</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">Ns</span> <span class="o">+</span> <span class="n">dvv</span>
        <span class="n">G</span> <span class="o">=</span> <span class="n">G_bar</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">grad_flags</span><span class="p">):</span>
            <span class="n">dG</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dG</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">Ns</span>

    <span class="c1"># Drop extra dims if Ns == 1</span>
    <span class="k">if</span> <span class="n">Ns</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">G</span> <span class="o">=</span> <span class="n">G</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">grad_flags</span><span class="p">):</span>
            <span class="n">dG</span> <span class="o">=</span> <span class="n">dG</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">separate_K</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">G</span><span class="p">,</span> <span class="n">dG</span><span class="p">,</span> <span class="n">varG</span><span class="p">,</span> <span class="n">dvarG</span><span class="p">,</span> <span class="n">var_ss</span><span class="p">,</span> <span class="n">I_sk</span><span class="p">,</span> <span class="n">J_sjk</span>
    <span class="k">return</span> <span class="n">G</span><span class="p">,</span> <span class="n">dG</span><span class="p">,</span> <span class="n">varG</span><span class="p">,</span> <span class="n">dvarG</span><span class="p">,</span> <span class="n">var_ss</span>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
      &copy; Copyright 2022, Machine and Human Intelligence research group (PI: Luigi Acerbi, University of Helsinki).<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>