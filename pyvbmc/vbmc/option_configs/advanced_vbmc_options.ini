[AdvancedOptions]
# Explicit noise handling (0: none; 1: unknown noise level; 2: user-provided noise)
uncertaintyhandling = []
# Array with indices of integer variables
integervars = []
# Base observation noise magnitude (standard deviation)
noisesize = []
# Max number of consecutive repeated measurements for noisy inputs
maxrepeatedobservations = 0
# Multiplicative discount True acquisition fcn to repeat measurement at the same location
repeatedacqdiscount = 1
# Number of initial target fcn evals
funevalstart = np.maximum(D, 10)
# Base step size for stochastic gradient descent
sgdstepsize = 0.005
# Skip active sampling the first iteration after warmup
skipactivesamplingafterwarmup = False
# Use ranking criterion to pick best non-converged solution
rankcriterion = True
# Required stable iterations to switch entropy approximation
tolstableentropyiters = 6
# Use variable component means for variational posterior
variablemeans = True
# Use variable mixture weight for variational posterior
variableweights = True
# Penalty multiplier for small mixture weights
weightpenalty = 0.1
# Run in diagnostics mode get additional info
diagnostics = False
# Output function
outputfcn = []
# Fraction of allowed exceptions when computing iteration stability
tolstableexcptfrac = 0.2
# Evaluated fcn values at X0
fvals = []
# Use Optimization Toolbox (if empty determine at runtime)
optimtoolbox = []
# Weighted proposal fcn for uncertainty search
proposalfcn = None
# Automatic nonlinear rescaling of variables
nonlinearscaling = True
# Fast search acquisition fcn(s)
searchacqfcn = ["@acqf_vbmc"]
# Samples for fast acquisition fcn eval per new point
nssearch = 2 ** 13
# Total samples for Monte Carlo approx. of the entropy
nsent = lambda K : 100 * K ** (2 / 3)
# Total samples for preliminary Monte Carlo approx. of the entropy
nsentfast = 0
# Total samples for refined Monte Carlo approx. of the entropy
nsentfine = lambda K : 2 ** 12 * K
# Total samples for Monte Carlo approx. of the entropy (final boost)
nsentboost = lambda K : 200 * K ** (2 / 3)
# Total samples for preliminary Monte Carlo approx. of the entropy (final boost)
nsentfastboost = []
# Total samples for refined Monte Carlo approx. of the entropy (final boost)
nsentfineboost = []
# Total samples for Monte Carlo approx. of the entropy (active sampling)
nsentactive = lambda K : 20 * K ** (2 / 3)
# Total samples for preliminary Monte Carlo approx. of the entropy (active sampling)
nsentfastactive = 0
# Total samples for refined Monte Carlo approx. of the entropy (active sampling)
nsentfineactive = lambda K : 200 * K
# Samples for fast approximation of the ELBO
nselbo = lambda K : 50 * K
# Multiplier to samples for fast approx. of ELBO for incremental iterations
nselboincr = 0.1
# Starting points to refine optimization of the ELBO
elbostarts = 2
# Max GP hyperparameter samples (decreases with training points)
nsgpmax = 80
# Max GP hyperparameter samples during warmup
nsgpmaxwarmup = 8
# Max GP hyperparameter samples during main algorithm
nsgpmaxmain = np.Inf
# Fcn evals without improvement before stopping warmup
warmupnoimprothreshold = 20 + 5 * D
# Also check for max fcn value improvement before stopping warmup
warmupcheckmax = True
# Force stable GP hyperparameter sampling (reduce samples or start optimizing)
stablegpsampling = 200 + 10 * D
# Force stable GP hyperparameter sampling after reaching this number of components
stablegpvpk = np.Inf
# Number of GP samples when GP is stable (0 = optimize)
stablegpsamples = 0
# Thinning for GP hyperparameter sampling
gpsamplethin = 5
# Initial design points for GP hyperparameter training
gptrainninit = 1024
# Final design points for GP hyperparameter training
gptrainninitfinal = 64
# Initial design method for GP hyperparameter training
gptraininitmethod = "rand"
# Tolerance for optimization of GP hyperparameters
gptolopt = 1e-5
# Tolerance for optimization of GP hyperparameters preliminary to MCMC
gptoloptmcmc = 1e-2
# Tolerance for optimization of GP hyperparameters during active sampling
gptoloptactive = 1e-4
# Tolerance for optimization of GP hyperparameters preliminary to MCMC during active sampling
gptoloptmcmcactive = 1e-2
# Threshold True GP variance used by regulatized acquisition fcns
tolgpvar = 1e-4
# Threshold True GP variance used to stabilize sampling
tolgpvarmcmc = 1e-4
# GP mean function
gpmeanfun = "negquad"
# GP integrated mean function
gpintmeanfun = 0
# Max variational components as a function of training points
kfunmax = lambda N : N ** (2 / 3)
# Variational components during warmup
kwarmup = 2
# Added variational components for stable solution
adaptivek = 2
# High Posterior Density region (fraction of training inputs)
hpdfrac = 0.8
# Uncertainty weight True ELCBO for computing lower bound improvement
elcboimproweight = 3
# Minimum fractional length scale
tollength = 1e-6
# Size of cache for storing fcn evaluations
cachesize = 500
# Fraction of search points from starting cache (if nonempty)
cachefrac = 0.5
# Stochastic optimizer for varational parameters
stochasticoptimizer = "adam"
# Stopping threshold for stochastic optimization
tolfunstochastic = 1e-3
# Max iterations for stochastic optimization
maxiterstochastic = 100 * (2 + D)
# Set stochastic optimization stepsize via GP hyperparameters
gpstochasticstepsize = False
# Tolerance True ELBO uncertainty for stopping (if variational posterior is stable)
tolsd = 0.1
# Stopping threshold True change of variational posterior per training point
tolskl = 0.01 * np.sqrt(D)
# Number of stable fcn evals for stopping warmup
tolstablewarmup = 15
# MCMC sampler for variational posteriors
variationalsampler = "malasample"
# Required ELCBO improvement per fcn eval before termination
tolimprovement = 0.01
# Use Gaussian approximation for symmetrized KL-divergence b\w iters
klgauss = True
# True mean of the target density (for debugging)
truemean = []
# True covariance of the target density (for debugging)
truecov = []
# Min number of fcn evals
minfunevals = 5 * D
# Min number of iterations
miniter = D
# Fraction of search points from heavy-tailed variational posterior
heavytailsearchfrac = 0.25
# Fraction of search points from multivariate normal
mvnsearchfrac = 0.25
# Fraction of search points from multivariate normal fitted to HPD points
hpdsearchfrac = 0
# Fraction of search points from uniform random box based True training inputs
boxsearchfrac = 0.25
# Fraction of search points from previous iterations
searchcachefrac = 0
# Always fully refit variational posterior
alwaysrefitvarpost = False
# Perform warm-up stage
warmup = True
# Special OPTIONS struct for warmup stage
warmupoptions = []
# Stop warm-up when ELCBO increase below threshold (per fcn eval)
stopwarmupthresh = 0.2
# Max log-likelihood difference for points kept after warmup
warmupkeepthreshold = 10 * D
# Max log-likelihood difference for points kept after a false-alarm warmup stop
warmupkeepthresholdfalsealarm = 100 * (D + 2)
# Reliability index required to stop warmup
stopwarmupreliability = 100
# Optimization method for active sampling
searchoptimizer = "cmaes"
# Initialize CMA-ES search SIGMA from variational posterior
searchcmaesvpinit = True
# Take bestever solution from CMA-ES search
searchcmaesbest = False
# Max number of acquisition fcn evaluations during search
searchmaxfunevals = 500 * (D + 2)
# Weight of previous trials (per trial) for running avg of variational posterior moments
momentsrunweight = 0.9
# Upper threshold True reliability index for full retraining of GP hyperparameters
gpretrainthreshold = 1
# Compute full ELCBO also at best midpoint
elcbomidpoint = True
# Multiplier to widths from previous posterior for GP sampling (Inf = do not use previous widths)
gpsamplewidths = 5
# Weight of previous trials (per trial) for running avg of GP hyperparameter covariance
hyprunweight = 0.9
# Use weighted hyperparameter posterior covariance
weightedhypcov = True
# Minimum weight for weighted hyperparameter posterior covariance
tolcovweight = 0
# MCMC sampler for GP hyperparameters
gphypsampler = "slicesample"
# Switch to covariance sampling below this threshold of stability index
covsamplethresh = 10
# Optimality tolerance for optimization of deterministic entropy
detenttolopt = 1e-3
# Switch from deterministic entropy to stochastic entropy when reaching stability
entropyswitch = False
# Force switch to stochastic entropy at this fraction of total fcn evals
entropyforceswitch = 0.8
# Alpha value for lower/upper deterministic entropy interpolation
detentropyalpha = 0
# Randomize deterministic entropy alpha during active sample updates
updaterandomalpha = False
# Online adaptation of alpha value for lower/upper deterministic entropy interpolation
adaptiveentropyalpha = False
# Start with deterministic entropy only with this number of vars or more
detentropymind = 5
# Fractional tolerance for constraint violation of variational parameters
tolconloss = 0.01
# SD multiplier of ELCBO for computing best variational solution
bestsafesd = 5
# When computing best solution lacking stability go back up to this fraction of iterations
bestfracback = 0.25
# Threshold mixture component weight for pruning
tolweight = 1e-2
# Multiplier to threshold for pruning mixture weights
pruningthresholdmultiplier = lambda K : 1 / np.sqrt(K)
# Annealing for hyperprior width of GP negative quadratic mean
annealedgpmean = lambda N,NMAX: 0
# Strict hyperprior for GP negative quadratic mean
constrainedgpmean = False
# Empirical Bayes prior over some GP hyperparameters
empiricalgpprior = False
# Minimum GP observation noise
tolgpnoise = np.sqrt(1e-5)
# Prior mean over GP input length scale (in plausible units)
gplengthpriormean = np.sqrt(D / 6)
# Prior std over GP input length scale (in plausible units)
gplengthpriorstd = 0.5 * np.log(1e3)
# Upper bound True GP input lengths based True plausible box (0 = ignore)
uppergplengthfactor = 0
# Initial samples (plausible is uniform in the plausible box)
initdesign = "plausible"
# Stricter upper bound True GP negative quadratic mean function
gpquadraticmeanbound = True
# Bandwidth parameter for GP smoothing (in units of plausible box)
bandwidth = 0
# Heuristic output warping (fitness shaping)
fitnessshaping = False
# Output warping starting threshold
outwarpthreshbase = 10 * D
# Output warping threshold multiplier when failed sub-threshold check
outwarpthreshmult = 1.25
# Output warping base threshold tolerance (fraction of current threshold)
outwarpthreshtol = 0.8
# Temperature for posterior tempering (allowed values T = 1234)
temperature = 1
# Use separate GP with constant mean for active search
separatesearchgp = False
# Discount observations from extremely low-density regions
noiseshaping = False
# Threshold from max observed value to start discounting
noiseshapingthreshold = 10 * D
# Proportionality factor of added noise wrt distance from threshold
noiseshapingfactor = 0.05
# Hedge True multiple acquisition functions
acqhedge = False
# Past iterations window to judge acquisition fcn improvement
acqhedgeiterwindow = 4
# Portfolio value decay per function evaluation
acqhedgedecay = 0.9
# MCMC variational steps before each active sampling
activevariationalsamples = 0
# Apply lower bound True variational components scale during variational sampling
scalelowerbound = True
# Perform variational optimization after each active sample
activesamplevpupdate = False
# Perform GP training after each active sample
activesamplegpupdate = False
# # iters past warmup to continue update after each active sample
activesamplefullupdatepastwarmup = 2
# Perform full update during active sampling if stability above threshold
activesamplefullupdatethreshold = 3
# Use previous variational posteriors to initialize optimization
variationalinitrepo = False
# Extra variational components sampled from GP profile
sampleextravpmeans = 0
# Uncertainty weight True ELCBO during active sampling
optimisticvariationalbound = 0
# # importance samples from smoothed variational posterior
activeimportancesamplingvpsamples = 100
# # importance samples from box-uniform centered True training inputs
activeimportancesamplingboxsamples = 100
# # importance samples through MCMC
activeimportancesamplingmcmcsamples = 100
# Thinning for importance sampling MCMC
activeimportancesamplingmcmcthin = 1
# fractional ESS threhsold to update GP and VP
activesamplefessthresh = 1
# % fractional ESS threhsold to do MCMC while active importance sampling
activeimportancesamplingfessthresh = 0.9
# Active search bound multiplier
activesearchbound = 2
# Try integrating GP mean function
integrategpmean = False
# Tolerance True closeness to bound constraints (fraction of total range)
tolboundx = 1e-5
# Recompute LCB max for each iteration based True current GP estimate
recomputelcbmax = True
# Input transform for bounded variables
boundedtransform = "logit"
# Use double GP
doublegp = False
# Warp every this number of iterations
warpeveryiters = 5
# Increase delay between warpings
incrementalwarpdelay = True
# Threshold True reliability index to perform warp
warptolreliability = 3
# Rotate and scale input
warprotoscaling = True
# Regularization weight towards diagonal covariance matrix for N training inputs
warpcovreg = 0
# Threshold True correlation matrix for roto-scaling
warprotocorrthresh = 0.05
# Min number of variational components to perform warp
warpmink = 5
# Immediately undo warp if not improving ELBO
warpundocheck = True
# Improvement of ELBO required to keep a warp proposal
warptolimprovement = 0.1
# Multiplier tolerance of ELBO SD after warp proposal
warptolsdmultiplier = 2
# Base tolerance True ELBO SD after warp proposal
warptolsdbase = 1
