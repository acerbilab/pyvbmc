{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Extended usage and output diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate extended usage of `pyvbmc`. We will:\n",
    "- take a brief look at `pyvbmc`'s diagnostic output,\n",
    "- show you how to continue optimization starting from the results of a previous run, and\n",
    "- show you how to save the results of optimization to disk.\n",
    "\n",
    "This notebook is Part 3 of a series of notebooks in which we present various example usages for VBMC with the `pyvbmc` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "from scipy.optimize import minimize\n",
    "from pyvbmc.vbmc import VBMC\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convergence Diagnostics\n",
    "\n",
    "For demonstration purposes, we will run VBMC with a restricted budget of function evaluations, insufficient to achieve convergence. Then we will inspect the output diagnostics, and resume optimization.\n",
    "\n",
    "We use a higher-dimensional analogue of the same toy target function in Example 1, a broad [Rosenbrock's banana function](https://en.wikipedia.org/wiki/Rosenbrock_function) in $D = 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 4  # A four-dimensional problem\n",
    "prior_mu = np.zeros(D)\n",
    "prior_var = 3 * np.ones(D)\n",
    "\n",
    "\n",
    "def log_prior(theta):\n",
    "    \"\"\"Multivariate normal prior on theta.\"\"\"\n",
    "    cov = np.diag(prior_var)\n",
    "    return scs.multivariate_normal(prior_mu, cov).logpdf(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood function of your model will in general depend on the observed data. This data can be fixed as a global variable, as we did directly above for `prior_mu` and `prior_var`. It can also be defined by a default second argument: to `pyvbmc` there is no difference so long as the function can be called with only a single argument (the parameters `theta`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta, data=np.ones(D)):\n",
    "    \"\"\"D-dimensional Rosenbrock's banana function.\"\"\"\n",
    "    # In this simple demo the data just translates the parameters:\n",
    "    theta = np.atleast_2d(theta)\n",
    "    theta = theta + data\n",
    "\n",
    "    x, y = theta[:, :-1], theta[:, 1:]\n",
    "    return -np.sum((x**2 - y) ** 2 + (x - 1) ** 2 / 100, axis=1)\n",
    "\n",
    "\n",
    "def log_joint(theta, data=np.ones(D)):\n",
    "    return log_likelihood(theta, data) + log_prior(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LB = np.full(D, -np.inf)  # Lower bounds\n",
    "UB = np.full(D, np.inf)  # Upper bounds\n",
    "PLB = np.full(D, prior_mu - np.sqrt(prior_var))  # Plausible lower bounds\n",
    "PUB = np.full(D, prior_mu + np.sqrt(prior_var))  # Plausible upper bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a typical inference scenario, we recommend starting from a \"good\" point (i.e. one near the mode). We can run a  quick preliminary optimization (though a more extensive optimization would not harm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x0 = np.random.uniform(PLB, PUB)  # Random point inside plausible box\n",
    "x0 = minimize(lambda t: -log_joint(t), x0).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping x0 to row vector.\n",
      "Reshaping lower bounds to (1, 4).\n",
      "Reshaping upper bounds to (1, 4).\n",
      "Reshaping plausible lower bounds to (1, 4).\n",
      "Reshaping plausible upper bounds to (1, 4).\n"
     ]
    }
   ],
   "source": [
    "# Limit number of function evaluations\n",
    "options = {\n",
    "    \"max_fun_evals\": 10 * D,\n",
    "}\n",
    "# We can specify either the log-joint, or the log-likelihood and log-prior.\n",
    "# In other words, the following lines are equivalent:\n",
    "vbmc = VBMC(\n",
    "    log_likelihood,\n",
    "    x0,\n",
    "    LB,\n",
    "    UB,\n",
    "    PLB,\n",
    "    PUB,\n",
    "    user_options=options,\n",
    "    log_prior=log_prior,\n",
    ")\n",
    "# vbmc = VBMC(\n",
    "#     log_joint,\n",
    "#     x0, LB, UB, PLB, PUB, user_options=options,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(`pyvbmc` expects the bounds to be `(1, D)` row vectors, and the initial point(s) to be of shape `(n, D)`, but it will accept and re-shape vectors of shape `(D,)` as well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning variational optimization assuming EXACT observations of the log-joint.\n",
      " Iteration  f-count    Mean[ELBO]    Std[ELBO]    sKL-iter[q]   K[q]  Convergence  Action\n",
      "     0         10          -5.13         3.81     98089.49        2        inf     start warm-up\n",
      "     1         15          -4.91         1.38         2.14        2        inf     \n",
      "     2         20          -3.21         1.48         1.80        2       40.7     \n",
      "     3         25          -4.58         1.27         1.45        2         33     \n",
      "     4         30          -5.33         0.30         0.83        2       17.3     \n",
      "     5         35          -4.86         1.31         1.90        2       37.6     \n",
      "     6         40          -4.61         0.76         1.63        2       30.6     \n",
      "   inf         40          -4.34         0.75         1.64       50       30.6     finalize\n",
      "Inference terminated: reached maximum number of function evaluations options.max_fun_evals.\n",
      "Estimated ELBO: -4.335 +/-0.749.\n",
      "Caution: Returned variational solution may have not converged.\n"
     ]
    }
   ],
   "source": [
    "vp, elbo, elbo_sd, success_flag, info = vbmc.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyvbmc` is warning us that convergence is doubtful. We can look at the output for more information and diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`False` means that `pyvbmc` has not converged to a stable solution within the given number of function evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function': '<function VBMC.__init__.<locals>.log_joint at 0x7f0bb9b81ee0>',\n",
       " 'problemtype': 'unconstrained',\n",
       " 'iterations': 6,\n",
       " 'funccount': 40,\n",
       " 'bestiter': 6,\n",
       " 'trainsetsize': 40,\n",
       " 'components': 50,\n",
       " 'rindex': 30.586103804330275,\n",
       " 'convergencestatus': 'no',\n",
       " 'overhead': nan,\n",
       " 'rngstate': 'rng',\n",
       " 'algorithm': 'Variational Bayesian Monte Carlo',\n",
       " 'version': '0.0.1',\n",
       " 'message': 'Inference terminated: reached maximum number of function evaluations options.max_fun_evals.',\n",
       " 'elbo': -4.335372122828826,\n",
       " 'elbo_sd': 0.7491560646729979}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `info` dictionary:\n",
    "- the `convergencestatus` field says 'no' (probable lack of convergence);\n",
    "- the reliability index `rindex` is 3.68, (should be less than 1).\n",
    "Our diagnostics tell us that this run has not converged, suggesting to increase the budget.\n",
    "\n",
    "Note that convergence to a solution does not mean that it is a _good_ solution. You should always check the returned variational posteriors, and ideally should compare across multiple runs of `pyvbmc`.\n",
    "\n",
    "## Continuing optimization from a previous result\n",
    "\n",
    "We can continue running `pyvbmc` where we left off by calling the `optimize()` method again. But first we will change the function evaluation budget to a more realistic value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning variational optimization assuming EXACT observations of the log-joint.\n",
      " Iteration  f-count    Mean[ELBO]    Std[ELBO]    sKL-iter[q]   K[q]  Convergence  Action\n",
      "     0         50          -6.16         1.85         0.60        2        inf     \n",
      "     1         55          -4.76         0.09         0.80        2        inf     \n",
      "     2         60          -4.66         0.03         0.05        2       1.35     \n",
      "     3         65          -4.58         0.01         0.03        3      0.762     \n",
      "     4         70          -4.55         0.01         0.01        4      0.303     \n",
      "     5         75          -4.51         0.01         0.01        5      0.325     \n",
      "     6         75          -4.20         0.16         0.55        6       10.7     rotoscale\n",
      "     7         80          -4.34         0.03         0.12        6       2.63     \n",
      "     8         85          -4.34         0.02         0.01        7      0.195     \n",
      "     9         90          -4.33         0.01         0.00       10      0.069     \n",
      "    10         95          -4.31         0.01         0.00       13       0.15     \n",
      "    11        100          -4.27         0.01         0.01       16      0.327     \n",
      "    12        105          -4.23         0.00         0.01       19      0.222     rotoscale, undo rotoscale\n",
      "    13        110          -4.22         0.00         0.00       22     0.0646     \n",
      "    14        115          -4.21         0.00         0.00       24     0.0654     stable\n",
      "   inf        115          -4.18         0.00         0.02       50     0.0654     finalize\n",
      "Inference terminated: variational solution stable for options.tol_stable_count fcn evaluations.\n",
      "Estimated ELBO: -4.177 +/-0.003.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(VariationalPosterior:\n",
       "     self.D = 4,\n",
       "     self.K = 50,\n",
       "     self.mu = (4, 50) ndarray,\n",
       "     self.w = (1, 50) ndarray,\n",
       "     self.sigma = (1, 50) ndarray,\n",
       "     self.lambd = \n",
       "         [[1.1971],\n",
       "          [0.9322],\n",
       "          [0.8981],\n",
       "          [0.9442]] : ndarray,\n",
       "     self.stats = <dict object at 0x7f0bb59b6f80>,\n",
       "     self._mode = None,\n",
       "     self.bounds = None,\n",
       "     self.delta = [[0., 0., 0., 0.]] : ndarray,\n",
       "     self.eta = (1, 50) ndarray,\n",
       "     self.gp = GP:\n",
       "         self.D = 4,\n",
       "         self.covariance = <gpyreg.covariance_functions.SquaredExponential object at 0x7f0bb5a978e0>,\n",
       "         self.mean = <gpyreg.mean_functions.NegativeQuadratic object at 0x7f0bb5a974c0>,\n",
       "         self.noise = <gpyreg.noise_functions.GaussianNoise object at 0x7f0bb5a97b20>,\n",
       "         self.X = (115, 4) ndarray,\n",
       "         self.y = (115, 1) ndarray,\n",
       "         self.s2 = None,\n",
       "         self.lower_bounds = (15,) ndarray,\n",
       "         self.upper_bounds = (15,) ndarray,\n",
       "         self.posteriors = \n",
       "             [<gpyreg.gaussian_process.Posterior object at 0x7f0bb5a97730>,\n",
       "              <gpyreg.gaussian_process.Posterior object at 0x7f0bb5a971c0>,\n",
       "              <gpyreg.gaussian_process.Posterior object at 0x7f0bb5a97520>,\n",
       "              <gpyreg.gaussian_process.Posterior object at 0x7f0bb5a97d00>,\n",
       "              <gpyreg.gaussian_process.Posterior object at 0x7f0bb5a974f0>,\n",
       "              <gpyreg.gaussian_process.Posterior object at 0x7f0bb5a97ca0>,\n",
       "              <gpyreg.gaussian_process.Posterior object at 0x7f0bb5a97580>] : ndarray,\n",
       "         self.hyper_priors = <dict object at 0x7f0bb9ee1a40>,\n",
       "         self.no_prior = False,\n",
       "         self.normalization_constants = (15,) ndarray,\n",
       "         self.temporary_data = <dict object at 0x7f0bb9b4fdc0>,\n",
       "     self.optimize_lambd = True,\n",
       "     self.optimize_mu = True,\n",
       "     self.optimize_sigma = True,\n",
       "     self.optimize_weights = True,\n",
       "     self.parameter_transformer = <pyvbmc.parameter_transformer.parameter_transformer.ParameterTransformer object at 0x7f0bb5a97850>,\n",
       " -4.177425458458755,\n",
       " 0.003017098068303876,\n",
       " True,\n",
       " {'function': '<function VBMC.__init__.<locals>.log_joint at 0x7f0bb9b81ee0>',\n",
       "  'problemtype': 'unconstrained',\n",
       "  'iterations': 14,\n",
       "  'funccount': 115,\n",
       "  'bestiter': 14,\n",
       "  'trainsetsize': 115,\n",
       "  'components': 50,\n",
       "  'rindex': 0.0654481971345285,\n",
       "  'convergencestatus': 'probable',\n",
       "  'overhead': nan,\n",
       "  'rngstate': 'rng',\n",
       "  'algorithm': 'Variational Bayesian Monte Carlo',\n",
       "  'version': '0.0.1',\n",
       "  'message': 'Inference terminated: variational solution stable for options.tol_stable_count fcn evaluations.',\n",
       "  'elbo': -4.177425458458755,\n",
       "  'elbo_sd': 0.003017098068303876})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This verbose syntax is required to avoid unintentionally modifying\n",
    "# options after initialization:\n",
    "vbmc.options.__setitem__(\"max_fun_evals\", 50 * (D + 2), force=True)\n",
    "vbmc.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saving results\n",
    "\n",
    "We can also save the `VBMC` instance to disk and reload it later, in order to continue optimization, sample from the posterior, check the results, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vbmc_test_save.pkl\", \"wb\") as f:\n",
    "    dill.dump(vbmc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.24485978, -0.4021069 ,  0.01427758,  1.00368226],\n",
       "        [-0.07017428, -0.06677729,  0.2468989 ,  0.26978397],\n",
       "        [-2.09081822,  0.4115007 ,  0.81232483,  2.03394574],\n",
       "        [-0.48901898,  0.20464576, -0.06871874, -0.18385511],\n",
       "        [-0.56658237, -0.77832803, -0.93870159, -0.06641562]]),\n",
       " array([13,  3, 31,  0,  2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"vbmc_test_save.pkl\", \"rb\") as f:\n",
    "    vbmc_2 = dill.load(f)\n",
    "vbmc_2.vp.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions\n",
    "\n",
    "In this notebook, we have given a brief overview of `pyvbmc`'s output diagnostics, and shown how to save, load, and resume optimization. \n",
    "\n",
    "In the next notebook, we will illustrate running `pyvbmc` multiple times in order to validate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: full code\n",
    "\n",
    "The following cell includes in a single place all the code used in Example 3, without the extra fluff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# skip this cell\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mscs\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False  # skip this cell\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "from scipy.optimize import minimize\n",
    "from pyvbmc.vbmc import VBMC\n",
    "import dill\n",
    "\n",
    "\n",
    "D = 4  # A four-dimensional problem\n",
    "prior_mu = np.zeros(D)\n",
    "prior_var = 3 * np.ones(D)\n",
    "\n",
    "\n",
    "def log_prior(theta):\n",
    "    \"\"\"Multivariate normal prior on theta.\"\"\"\n",
    "    cov = np.diag(prior_var)\n",
    "    return scs.multivariate_normal(prior_mu, cov).logpdf(theta)\n",
    "\n",
    "\n",
    "def log_likelihood(theta, data=np.ones(D)):\n",
    "    \"\"\"D-dimensional Rosenbrock's banana function.\"\"\"\n",
    "    # In this simple demo the data just translates the parameters:\n",
    "    theta = np.atleast_2d(theta)\n",
    "    theta = theta + data\n",
    "\n",
    "    x, y = theta[:, :-1], theta[:, 1:]\n",
    "    return -np.sum((x**2 - y) ** 2 + (x - 1) ** 2 / 100, axis=1)\n",
    "\n",
    "\n",
    "def log_joint(theta, data=np.ones(D)):\n",
    "    return log_likelihood(theta, data) + log_prior(theta)\n",
    "\n",
    "\n",
    "LB = np.full(D, -np.inf)  # Lower bounds\n",
    "UB = np.full(D, np.inf)  # Upper bounds\n",
    "PLB = np.full(D, prior_mu - np.sqrt(prior_var))  # Plausible lower bounds\n",
    "PUB = np.full(D, prior_mu + np.sqrt(prior_var))  # Plausible upper bounds\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "x0 = np.random.uniform(PLB, PUB)  # Random point inside plausible box\n",
    "x0 = minimize(lambda t: -log_joint(t), x0).x\n",
    "\n",
    "\n",
    "# Limit number of function evaluations\n",
    "options = {\n",
    "    \"max_fun_evals\": 10 * D,\n",
    "}\n",
    "# We can specify either the log-joint, or the log-likelihood and log-prior.\n",
    "# In other words, the following lines are equivalent:\n",
    "vbmc = VBMC(\n",
    "    log_likelihood,\n",
    "    x0,\n",
    "    LB,\n",
    "    UB,\n",
    "    PLB,\n",
    "    PUB,\n",
    "    user_options=options,\n",
    "    log_prior=log_prior,\n",
    ")\n",
    "# vbmc = VBMC(\n",
    "#     log_joint,\n",
    "#     x0, LB, UB, PLB, PUB, user_options=options,\n",
    "# )\n",
    "\n",
    "\n",
    "vp, elbo, elbo_sd, success_flag, info = vbmc.optimize()\n",
    "\n",
    "\n",
    "success_flag\n",
    "\n",
    "\n",
    "info\n",
    "\n",
    "\n",
    "# This verbose syntax is required to avoid unintentionally modifying\n",
    "# options after initialization:\n",
    "vbmc.options.__setitem__(\"max_fun_evals\", 50 * (D + 2), force=True)\n",
    "vbmc.optimize()\n",
    "\n",
    "\n",
    "with open(\"vbmc_test_save.pkl\", \"wb\") as f:\n",
    "    dill.dump(vbmc, f)\n",
    "\n",
    "\n",
    "with open(\"vbmc_test_save.pkl\", \"rb\") as f:\n",
    "    vbmc_2 = dill.load(f)\n",
    "vbmc_2.vp.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgments\n",
    "\n",
    "Work on the `pyvbmc` package was funded by the [Finnish Center for Artificial Intelligence FCAI](https://fcai.fi/)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf2c3e35bb9d622e963fe7adafe5d3d77a0ee2382f730f35475e9a620896d84b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
