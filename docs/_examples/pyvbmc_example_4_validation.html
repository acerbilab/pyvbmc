
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>PyVBMC Example 4: Multiple runs as validation &#8212; PyVBMC</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=84ace793992934648b4de8eed757e5a2" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.9d8b4a8b9bb19db25eeaddc40d639ba2.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="PyVBMC Example 5: Noisy log-likelihood evaluations" href="pyvbmc_example_5_noisy_likelihoods.html" />
    <link rel="prev" title="PyVBMC Example 3: Output diagnostics and saving results" href="pyvbmc_example_3_diagnostics_and_saving.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<div class="col-12 col-md-3 bd-sidebar site-navigation " id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">PyVBMC</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../quickstart.html">
   Getting started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../api/classes/vbmc.html">
   VBMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../api/classes/variational_posterior.html">
   VariationalPosterior
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../api/options/vbmc_options.html">
   VBMC options
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../api/advanced_docs.html">
   Advanced documentation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/acquisition_functions.html">
     Acquisition Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/function_logger.html">
     FunctionLogger
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/iteration_history.html">
     IterationHistory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/options.html">
     Options
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/parameter_transformer.html">
     ParameterTransformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/timer.html">
     Timer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/variational_posterior.html">
     VariationalPosterior
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/vbmc.html">
     VBMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/functions/active_sample.html">
     active_sample
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/functions/create_vbmc_animation.html">
     create_vbmc_animation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/functions/decorators.html">
     decorators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/functions/entropy.html">
     entropy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/functions/get_hpd.html">
     get_hpd
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/functions/kde_1d.html">
     kde_1d
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/functions/whitening.html">
     whitening
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/options/vbmc_options.html">
     VBMC options
    </a>
   </li>
  </ul>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="pyvbmc_example_1_basic_usage.html">
   PyVBMC Example 1: Basic usage
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pyvbmc_example_2_inputs_outputs.html">
   PyVBMC Example 2: Understanding the inputs and the output trace
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pyvbmc_example_3_diagnostics_and_saving.html">
   PyVBMC Example 3: Output diagnostics and saving results
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   PyVBMC Example 4: Multiple runs as validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pyvbmc_example_5_noisy_likelihoods.html">
   PyVBMC Example 5: Noisy log-likelihood evaluations
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../development.html">
   Instructions for developers and contributors
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../about_us.html">
   About us
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>


          


          
<!-- This is an invisible pixel that we watch to see if we've scrolled. -->
<div class="sbt-scroll-pixel-helper"></div>
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            <div class="topbar-left">
                
                <label class="nav-toggle-button" for="__navigation">
                    <div class="visually-hidden">Toggle navigation</div>
                    <i class="fas fa-bars"></i>
                </label>
                
            </div>
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/_examples/pyvbmc_example_4_validation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-definition">
   1. Model definition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#validation">
   2. Validation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusions">
   3. Conclusions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-4-full-code">
   Example 4: full code
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#acknowledgments">
   Acknowledgments
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>PyVBMC Example 4: Multiple runs as validation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-definition">
   1. Model definition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#validation">
   2. Validation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusions">
   3. Conclusions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-4-full-code">
   Example 4: full code
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#acknowledgments">
   Acknowledgments
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="pyvbmc-example-4-multiple-runs-as-validation">
<h1>PyVBMC Example 4: Multiple runs as validation<a class="headerlink" href="#pyvbmc-example-4-multiple-runs-as-validation" title="Permalink to this headline">¶</a></h1>
<p>Here we explore running PyVBMC multiple times in order to validate the results: if the final variational posteriors obtained from several different initial points are consistent, we can be more confident that the results are correct.</p>
<p>This notebook is Part 4 of a series of notebooks in which we present various example usages for VBMC with the PyVBMC package.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">scs</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
<span class="kn">from</span> <span class="nn">pyvbmc.vbmc</span> <span class="kn">import</span> <span class="n">VBMC</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">dill</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="model-definition">
<h2>1. Model definition<a class="headerlink" href="#model-definition" title="Permalink to this headline">¶</a></h2>
<p>We use the same toy target function as in Example 1, a broad <a class="reference external" href="https://en.wikipedia.org/wiki/Rosenbrock_function">Rosenbrock’s banana function</a> in <span class="math notranslate nohighlight">\(D = 2\)</span>, with unbounded parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># We&#39;ll use a 2-D problem for a quicker demonstration</span>
<span class="n">prior_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="n">prior_var</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Multivariate normal prior on theta.&quot;&quot;&quot;</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">prior_var</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scs</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">prior_mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">)</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>


<span class="c1"># log-likelihood (Rosenbrock)</span>
<span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;D-dimensional Rosenbrock&#39;s banana function.&quot;&quot;&quot;</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">theta</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">100</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="c1"># Full model:</span>
<span class="k">def</span> <span class="nf">log_joint</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">D</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;log-density of the joint distribution.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">log_likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_prior</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>


<span class="n">LB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>  <span class="c1"># Lower bounds</span>
<span class="n">UB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>  <span class="c1"># Upper bounds</span>
<span class="n">PLB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">prior_mu</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">prior_var</span><span class="p">))</span>  <span class="c1"># Plausible lower bounds</span>
<span class="n">PUB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">prior_mu</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">prior_var</span><span class="p">))</span>  <span class="c1"># Plausible upper bounds</span>
<span class="n">options</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;max_fun_evals&quot;</span><span class="p">:</span> <span class="mi">50</span> <span class="o">*</span> <span class="n">D</span>  <span class="c1"># Slightly reduced from 50 * (D + 2), for speed</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="validation">
<h2>2. Validation<a class="headerlink" href="#validation" title="Permalink to this headline">¶</a></h2>
<p>For validation, we recommend running PyVBMC 3-4 times from different initial points:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_runs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">vps</span><span class="p">,</span> <span class="n">elbos</span><span class="p">,</span> <span class="n">elbo_sds</span><span class="p">,</span> <span class="n">success_flags</span><span class="p">,</span> <span class="n">result_dicts</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_runs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PyVBMC run </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> of </span><span class="si">{</span><span class="n">n_runs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Determine initial point x0:</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">x0</span> <span class="o">=</span> <span class="n">prior_mu</span>  <span class="c1"># First run, start from prior mean</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">PLB</span><span class="p">,</span> <span class="n">PUB</span><span class="p">)</span>  <span class="c1"># Other runs, randomize</span>
    <span class="c1"># Preliminary maximum a posteriori (MAP) estimation:</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="o">-</span><span class="n">log_joint</span><span class="p">(</span><span class="n">t</span><span class="p">),</span>
        <span class="n">x0</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>  <span class="c1"># minimize expects 1-D input for initial point x0</span>
        <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)],</span>
    <span class="p">)</span><span class="o">.</span><span class="n">x</span>

    <span class="c1"># Run PyVBMC:</span>
    <span class="n">vbmc</span> <span class="o">=</span> <span class="n">VBMC</span><span class="p">(</span>
        <span class="n">log_joint</span><span class="p">,</span>
        <span class="n">x0</span><span class="p">,</span>
        <span class="n">LB</span><span class="p">,</span>
        <span class="n">UB</span><span class="p">,</span>
        <span class="n">PLB</span><span class="p">,</span>
        <span class="n">PUB</span><span class="p">,</span>
        <span class="n">user_options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">vp</span><span class="p">,</span> <span class="n">elbo</span><span class="p">,</span> <span class="n">elbo_sd</span><span class="p">,</span> <span class="n">success_flag</span><span class="p">,</span> <span class="n">result_dict</span> <span class="o">=</span> <span class="n">vbmc</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>

    <span class="c1"># Record the results:</span>
    <span class="n">vps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vp</span><span class="p">)</span>
    <span class="n">elbos</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elbo</span><span class="p">)</span>
    <span class="n">elbo_sds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elbo_sd</span><span class="p">)</span>
    <span class="n">success_flags</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">success_flag</span><span class="p">)</span>
    <span class="n">result_dicts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>PyVBMC run 0 of 3
Reshaping x0 to row vector.
Beginning variational optimization assuming EXACT observations of the log-joint.
 Iteration  f-count    Mean[ELBO]    Std[ELBO]    sKL-iter[q]   K[q]  Convergence  Action
     0         10          -1.79         1.06     57003.37        2        inf     start warm-up
     1         15          -1.84         0.06         0.65        2        inf     
     2         20          -1.81         0.00         0.09        2       2.33     
     3         25          -1.81         0.00         0.00        2     0.0355     
     4         30          -1.84         0.00         0.00        2      0.158     end warm-up
     5         35          -1.81         0.00         0.00        2      0.185     
     6         40          -1.81         0.00         0.00        2      0.041     
     7         45          -1.64         0.00         0.06        5        1.9     
     8         50          -1.62         0.00         0.01        6      0.222     rotoscale, undo rotoscale
     9         55          -1.59         0.00         0.01        9      0.244     
    10         60          -1.55         0.00         0.01       12      0.394     
    11         65          -1.54         0.00         0.00       15     0.0544     
    12         70          -1.54         0.00         0.00       16     0.0236     
    13         75          -1.55         0.00         0.00       14     0.0289     stable
   inf         75          -1.53         0.00         0.00       50     0.0289     finalize
Inference terminated: variational solution stable for options.tol_stable_count fcn evaluations.
Estimated ELBO: -1.527 +/-0.000.
PyVBMC run 1 of 3
Reshaping x0 to row vector.
Beginning variational optimization assuming EXACT observations of the log-joint.
 Iteration  f-count    Mean[ELBO]    Std[ELBO]    sKL-iter[q]   K[q]  Convergence  Action
     0         10          -0.97         0.80     60421.09        2        inf     start warm-up
     1         15          -1.88         0.38         0.42        2        inf     
     2         20          -1.83         0.09         0.51        2       12.6     
     3         25          -1.81         0.00         0.04        2      0.984     
     4         30          -1.79         0.00         0.00        2      0.127     end warm-up
     5         35          -1.80         0.00         0.00        2     0.0361     
     6         40          -1.80         0.00         0.00        2     0.0944     
     7         45          -1.71         0.00         0.04        5       1.18     
     8         50          -1.67         0.00         0.01        6      0.227     rotoscale, undo rotoscale
     9         55          -1.65         0.00         0.01        9      0.301     
    10         60          -1.62         0.00         0.01       12      0.203     
    11         65          -1.61         0.00         0.01       14      0.178     
    12         70          -1.58         0.00         0.00       14      0.112     
    13         75          -1.55         0.00         0.01       14      0.385     stable
   inf         75          -1.53         0.00         0.02       50      0.385     finalize
Inference terminated: variational solution stable for options.tol_stable_count fcn evaluations.
Estimated ELBO: -1.527 +/-0.001.
PyVBMC run 2 of 3
Reshaping x0 to row vector.
Beginning variational optimization assuming EXACT observations of the log-joint.
 Iteration  f-count    Mean[ELBO]    Std[ELBO]    sKL-iter[q]   K[q]  Convergence  Action
     0         10          -1.36         1.10     56296.06        2        inf     start warm-up
     1         15          -1.80         0.10         0.36        2        inf     
     2         20          -1.80         0.00         0.06        2       1.47     
     3         25          -1.82         0.00         0.01        2       0.24     
     4         30          -1.81         0.00         0.00        2      0.088     end warm-up
     5         35          -1.78         0.00         0.13        2       3.05     
     6         40          -1.77         0.00         0.00        2      0.148     
     7         45          -1.64         0.00         0.09        5        2.5     
     8         50          -1.63         0.00         0.00        6      0.063     rotoscale, undo rotoscale
     9         55          -1.55         0.00         0.03        9          1     
    10         60          -1.55         0.00         0.00       10      0.052     
    11         65          -1.55         0.00         0.00       12     0.0085     
    12         70          -1.54         0.00         0.00       12     0.0414     
    13         75          -1.55         0.00         0.00       11     0.0776     
    14         80          -1.54         0.00         0.00       10     0.0524     
    15         85          -1.54         0.00         0.00       10     0.0137     
    16         90          -1.54         0.00         0.00       11     0.0239     
    17         95          -1.53         0.00         0.00       10      0.047     stable
   inf         95          -1.53         0.00         0.00       50      0.047     finalize
Inference terminated: variational solution stable for options.tol_stable_count fcn evaluations.
Estimated ELBO: -1.525 +/-0.000.
</pre></div>
</div>
</div>
</div>
<p>We now perform some diagnostic checks on the variational solution. First, we check that the ELBO’s from different runs are close to each other (i.e. with a difference much smaller than 1):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">elbos</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-1.5266615089493838, -1.5266716056530365, -1.525455555758563]
</pre></div>
</div>
</div>
</div>
<p>Then, we check that the variational posteriors from distinct runs are similar. As a measure of similarity, we compute the Kullback-Leibler divergence between each pair:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kl_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_runs</span><span class="p">,</span> <span class="n">n_runs</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_runs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">n_runs</span><span class="p">):</span>
        <span class="c1"># The `kldiv` method computes the divergence in both directions:</span>
        <span class="n">kl_ij</span><span class="p">,</span> <span class="n">kl_ji</span> <span class="o">=</span> <span class="n">vps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span><span class="n">vp2</span><span class="o">=</span><span class="n">vps</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="n">kl_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">kl_ij</span>
        <span class="n">kl_matrix</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">kl_ji</span>
</pre></div>
</div>
</div>
</div>
<p>Note that the KL divergence is asymmetric, so we have an asymmetric matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">kl_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[-0.          0.00834495  0.0042796 ]
 [ 0.0077242  -0.          0.01210863]
 [ 0.00665952  0.0172008  -0.        ]]
</pre></div>
</div>
</div>
</div>
<p>Ideally, we want all KL divergence matrix entries to be much smaller than 1. For a qualitative validation, we recommend a visual inspection of the posteriors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">vp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vps</span><span class="p">):</span>
    <span class="n">samples</span><span class="p">,</span> <span class="n">__</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">vps</span><span class="p">),</span>
        <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;VP </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;Variational Posterior Samples&quot;&quot;&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pyvbmc_example_4_validation_14_0.png" src="../_images/pyvbmc_example_4_validation_14_0.png" />
</div>
</div>
<p>We can also check that convergence was acheived in all runs, according to PyVBMC (we want <code class="docutils literal notranslate"><span class="pre">success_flag</span> <span class="pre">=</span> <span class="pre">True</span></code> for each run):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">success_flags</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[True, True, True]
</pre></div>
</div>
</div>
</div>
<p>Finally, we can pick the variational solution with highest ELCBO (the lower confidence bound on the ELBO).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">beta_lcb</span> <span class="o">=</span> <span class="mf">3.0</span>  <span class="c1"># Standard confidence parameter (in standard deviations)</span>
<span class="c1"># beta_lcb = 5.0  # This is more conservative</span>
<span class="n">elcbos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">elbos</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta_lcb</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">elbo_sds</span><span class="p">)</span>
<span class="n">idx_best</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">elcbos</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">idx_best</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2
</pre></div>
</div>
</div>
</div>
<p>We’ll save this best posterior to a file, so we can compare it against our results in the next notebook:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;noise_free_vp.pkl&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">dill</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">vps</span><span class="p">[</span><span class="n">idx_best</span><span class="p">],</span> <span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="conclusions">
<h2>3. Conclusions<a class="headerlink" href="#conclusions" title="Permalink to this headline">¶</a></h2>
<p>In this notebook, we have shown you some techniques to validate the results of PyVBMC by comparing statitics across multiple runs.</p>
<p>In the next notebook, we will demonstrate using PyVBMC with models in which the likelihood function is noisy, or can only be estimated up to some noise.</p>
</div>
<div class="section" id="example-4-full-code">
<h2>Example 4: full code<a class="headerlink" href="#example-4-full-code" title="Permalink to this headline">¶</a></h2>
<p>The following cell includes in a single place all the code used in Example 4, without the extra fluff.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span> <span class="kc">False</span>  <span class="c1"># skip this cell</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">scs</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
<span class="kn">from</span> <span class="nn">pyvbmc.vbmc</span> <span class="kn">import</span> <span class="n">VBMC</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">dill</span>


<span class="n">D</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># We&#39;ll use a 2-D problem for a quicker demonstration</span>
<span class="n">prior_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="n">prior_var</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Multivariate normal prior on theta.&quot;&quot;&quot;</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">prior_var</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scs</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">prior_mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">)</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>


<span class="c1"># log-likelihood (Rosenbrock)</span>
<span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;D-dimensional Rosenbrock&#39;s banana function.&quot;&quot;&quot;</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">theta</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">100</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="c1"># Full model:</span>
<span class="k">def</span> <span class="nf">log_joint</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">D</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;log-density of the joint distribution.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">log_likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_prior</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>


<span class="n">LB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>  <span class="c1"># Lower bounds</span>
<span class="n">UB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>  <span class="c1"># Upper bounds</span>
<span class="n">PLB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">prior_mu</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">prior_var</span><span class="p">))</span>  <span class="c1"># Plausible lower bounds</span>
<span class="n">PUB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">prior_mu</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">prior_var</span><span class="p">))</span>  <span class="c1"># Plausible upper bounds</span>
<span class="n">options</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;max_fun_evals&quot;</span><span class="p">:</span> <span class="mi">50</span> <span class="o">*</span> <span class="n">D</span>  <span class="c1"># Slightly reduced from 50 * (D + 2), for speed</span>
<span class="p">}</span>


<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_runs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">vps</span><span class="p">,</span> <span class="n">elbos</span><span class="p">,</span> <span class="n">elbo_sds</span><span class="p">,</span> <span class="n">success_flags</span><span class="p">,</span> <span class="n">result_dicts</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_runs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PyVBMC run </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> of </span><span class="si">{</span><span class="n">n_runs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Determine initial point x0:</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">x0</span> <span class="o">=</span> <span class="n">prior_mu</span>  <span class="c1"># First run, start from prior mean</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">PLB</span><span class="p">,</span> <span class="n">PUB</span><span class="p">)</span>  <span class="c1"># Other runs, randomize</span>
    <span class="c1"># Preliminary maximum a posteriori (MAP) estimation:</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="o">-</span><span class="n">log_joint</span><span class="p">(</span><span class="n">t</span><span class="p">),</span>
        <span class="n">x0</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>  <span class="c1"># minimize expects 1-D input for initial point x0</span>
        <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)],</span>
    <span class="p">)</span><span class="o">.</span><span class="n">x</span>

    <span class="c1"># Run PyVBMC:</span>
    <span class="n">vbmc</span> <span class="o">=</span> <span class="n">VBMC</span><span class="p">(</span>
        <span class="n">log_joint</span><span class="p">,</span>
        <span class="n">x0</span><span class="p">,</span>
        <span class="n">LB</span><span class="p">,</span>
        <span class="n">UB</span><span class="p">,</span>
        <span class="n">PLB</span><span class="p">,</span>
        <span class="n">PUB</span><span class="p">,</span>
        <span class="n">user_options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">vp</span><span class="p">,</span> <span class="n">elbo</span><span class="p">,</span> <span class="n">elbo_sd</span><span class="p">,</span> <span class="n">success_flag</span><span class="p">,</span> <span class="n">result_dict</span> <span class="o">=</span> <span class="n">vbmc</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>

    <span class="c1"># Record the results:</span>
    <span class="n">vps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vp</span><span class="p">)</span>
    <span class="n">elbos</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elbo</span><span class="p">)</span>
    <span class="n">elbo_sds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elbo_sd</span><span class="p">)</span>
    <span class="n">success_flags</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">success_flag</span><span class="p">)</span>
    <span class="n">result_dicts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result_dict</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="n">elbos</span><span class="p">)</span>


<span class="n">kl_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_runs</span><span class="p">,</span> <span class="n">n_runs</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_runs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">n_runs</span><span class="p">):</span>
        <span class="c1"># The `kldiv` method computes the divergence in both directions:</span>
        <span class="n">kl_ij</span><span class="p">,</span> <span class="n">kl_ji</span> <span class="o">=</span> <span class="n">vps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span><span class="n">vp2</span><span class="o">=</span><span class="n">vps</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="n">kl_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">kl_ij</span>
        <span class="n">kl_matrix</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">kl_ji</span>


<span class="nb">print</span><span class="p">(</span><span class="n">kl_matrix</span><span class="p">)</span>


<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">vp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vps</span><span class="p">):</span>
    <span class="n">samples</span><span class="p">,</span> <span class="n">__</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">vps</span><span class="p">),</span>
        <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;VP </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;Variational Posterior Samples&quot;&quot;&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>


<span class="nb">print</span><span class="p">(</span><span class="n">success_flags</span><span class="p">)</span>


<span class="n">beta_lcb</span> <span class="o">=</span> <span class="mf">3.0</span>  <span class="c1"># Standard confidence parameter (in standard deviations)</span>
<span class="c1"># beta_lcb = 5.0  # This is more conservative</span>
<span class="n">elcbos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">elbos</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta_lcb</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">elbo_sds</span><span class="p">)</span>
<span class="n">idx_best</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">elcbos</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">idx_best</span><span class="p">)</span>


<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;noise_free_vp.pkl&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">dill</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">vps</span><span class="p">[</span><span class="n">idx_best</span><span class="p">],</span> <span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">AssertionError</span><span class="g g-Whitespace">                            </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span> <span class="p">[</span><span class="mi">11</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="k">assert</span> <span class="kc">False</span>  <span class="c1"># skip this cell</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">scs</span>

<span class="ne">AssertionError</span>: 
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="acknowledgments">
<h2>Acknowledgments<a class="headerlink" href="#acknowledgments" title="Permalink to this headline">¶</a></h2>
<p>Work on the PyVBMC package was funded by the <a class="reference external" href="https://fcai.fi/">Finnish Center for Artificial Intelligence FCAI</a>.</p>
</div>
</div>


              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="pyvbmc_example_3_diagnostics_and_saving.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">PyVBMC Example 3: Output diagnostics and saving results</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="pyvbmc_example_5_noisy_likelihoods.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">PyVBMC Example 5: Noisy log-likelihood evaluations</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
        &copy; Copyright 2022, Machine and Human Intelligence research group (PI: Luigi Acerbi, University of Helsinki).<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>