
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>PyVBMC Example 1: Basic usage &#8212; PyVBMC</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://acerbilab.github.io/pyvbmc/_examples/pyvbmc_example_1_basic_usage.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="PyVBMC Example 2: Understanding the inputs and the output trace" href="pyvbmc_example_2_inputs_outputs.html" />
    <link rel="prev" title="whitening" href="../api/functions/whitening.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.svg" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../quickstart.html">
   Getting started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../api/classes/vbmc.html">
   VBMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../api/classes/variational_posterior.html">
   VariationalPosterior
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../api/options/vbmc_options.html">
   VBMC options
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../api/advanced_docs.html">
   Advanced documentation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/acquisition_functions.html">
     Acquisition Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/function_logger.html">
     FunctionLogger
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/iteration_history.html">
     IterationHistory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/options.html">
     Options
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/parameter_transformer.html">
     ParameterTransformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/timer.html">
     Timer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/variational_posterior.html">
     VariationalPosterior
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/classes/vbmc.html">
     VBMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/functions/active_sample.html">
     active_sample
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/functions/create_vbmc_animation.html">
     create_vbmc_animation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/functions/decorators.html">
     decorators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/functions/entropy.html">
     entropy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/functions/get_hpd.html">
     get_hpd
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/functions/kde_1d.html">
     kde_1d
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/functions/whitening.html">
     whitening
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/options/vbmc_options.html">
     VBMC options
    </a>
   </li>
  </ul>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   PyVBMC Example 1: Basic usage
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pyvbmc_example_2_inputs_outputs.html">
   PyVBMC Example 2: Understanding the inputs and the output trace
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pyvbmc_example_3_diagnostics_and_saving.html">
   PyVBMC Example 3: Output diagnostics and saving results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pyvbmc_example_4_validation.html">
   PyVBMC Example 4: Multiple runs as validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pyvbmc_example_5_noisy_likelihoods.html">
   PyVBMC Example 5: Noisy log-likelihood evaluations
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../development.html">
   Instructions for developers and contributors
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../about_us.html">
   About us
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/acerbilab/pyvbmc/main?urlpath=lab/tree/_examples/pyvbmc_example_1_basic_usage.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/acerbilab/pyvbmc/blob/main/_examples/pyvbmc_example_1_basic_usage.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/acerbilab/pyvbmc"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/acerbilab/pyvbmc/issues/new?title=Issue%20on%20page%20%2F_examples/pyvbmc_example_1_basic_usage.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/acerbilab/pyvbmc/edit/main/_examples/pyvbmc_example_1_basic_usage.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/_examples/pyvbmc_example_1_basic_usage.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-the-goal-of-py-vbmc">
   0. What is the goal of (Py)VBMC?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-definition-log-likelihood-log-prior-and-log-joint">
   1. Model definition: log-likelihood, log-prior, and log-joint
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parameter-setup-bounds-and-starting-point">
   2. Parameter setup: bounds and starting point
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#initialize-and-run-inference">
   3. Initialize and run inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#remarks">
     Remarks:
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#examine-and-visualize-results">
   4. Examine and visualize results
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusions">
   5. Conclusions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#acknowledgments">
   Acknowledgments
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>PyVBMC Example 1: Basic usage</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-the-goal-of-py-vbmc">
   0. What is the goal of (Py)VBMC?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-definition-log-likelihood-log-prior-and-log-joint">
   1. Model definition: log-likelihood, log-prior, and log-joint
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parameter-setup-bounds-and-starting-point">
   2. Parameter setup: bounds and starting point
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#initialize-and-run-inference">
   3. Initialize and run inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#remarks">
     Remarks:
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#examine-and-visualize-results">
   4. Examine and visualize results
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusions">
   5. Conclusions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#acknowledgments">
   Acknowledgments
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="pyvbmc-example-1-basic-usage">
<h1>PyVBMC Example 1: Basic usage<a class="headerlink" href="#pyvbmc-example-1-basic-usage" title="Permalink to this headline">#</a></h1>
<p>In this introductory example, we will show a simple usage of Variational Bayesian Monte Carlo (VBMC) to perform Bayesian inference on a synthetic target.</p>
<p>This notebook is Part 1 of a series of notebooks in which we present various example usages for VBMC with the PyVBMC package. The code used in this example is available as a script <a class="reference external" href="https://github.com/acerbilab/pyvbmc/blob/main/examples/pyvbmc_example_1_full_code.py">here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">scs</span>
<span class="kn">from</span> <span class="nn">pyvbmc</span> <span class="kn">import</span> <span class="n">VBMC</span>
</pre></div>
</div>
</div>
</div>
<section id="what-is-the-goal-of-py-vbmc">
<h2>0. What is the goal of (Py)VBMC?<a class="headerlink" href="#what-is-the-goal-of-py-vbmc" title="Permalink to this headline">#</a></h2>
<p>VBMC is an algorithm to efficiently perform Bayesian inference, and PyVBMC is its Python implementation. We recall that the goal of Bayesian inference is to obtain the <em>posterior distribution</em> <span class="math notranslate nohighlight">\(p(\mathbf{x}|\mathcal{D})\)</span>  for a vector of model parameters <span class="math notranslate nohighlight">\(\textbf{x}\)</span> and data set <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. The calculation involves <a class="reference external" href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ theorem</a>:</p>
<div class="math notranslate nohighlight">
\[
p(\mathbf{x}|\mathcal{D}) = \frac{p(\mathcal{D}|\mathbf{x}) p(\mathbf{x})}{p(\mathcal{D})},
\]</div>
<p>where <span class="math notranslate nohighlight">\(p(\mathcal{D}|\mathbf{x})\)</span> is the <em>likelihood</em>, <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span> is the <em>prior</em>, and <span class="math notranslate nohighlight">\(p(\mathcal{D})\)</span> is the normalization constant, also known as the <em>model evidence</em> or <em>marginal likelihood</em>.</p>
<p>VBMC takes as input the unnormalized log posterior, or log (joint) density, <span class="math notranslate nohighlight">\(\log p(\mathcal{D}|\mathbf{x}) p(\mathbf{x})\)</span> (also known as the <em>target</em>), and outputs:</p>
<ul class="simple">
<li><p>a <em>variational posterior</em> <span class="math notranslate nohighlight">\(q(\textbf{x}|\mathcal{D})\)</span> which is an (often very accurate) approximation of the true posterior;</p></li>
<li><p>an approximate lower bound to the log marginal likelihood, called ELBO (<strong>e</strong>vidence <strong>l</strong>ower <strong>bo</strong>und), and an estimate of its uncertainty;</p></li>
</ul>
<p>all using as few target evaluations as possible.
In the following, we see how to set up and run PyVBMC on a toy example.</p>
</section>
<section id="model-definition-log-likelihood-log-prior-and-log-joint">
<h2>1. Model definition: log-likelihood, log-prior, and log-joint<a class="headerlink" href="#model-definition-log-likelihood-log-prior-and-log-joint" title="Permalink to this headline">#</a></h2>
<p>Normally, the log-likelihood function <span class="math notranslate nohighlight">\(\log p(\mathcal{D}|\mathbf{x})\)</span> depends on the model under consideration, where <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is the data and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a vector of <span class="math notranslate nohighlight">\(D\)</span> model parameters.
For this example, we set as toy log-likelihood for our model a broad <a class="reference external" href="https://en.wikipedia.org/wiki/Rosenbrock_function">Rosenbrock’s banana function</a> in <span class="math notranslate nohighlight">\(D = 2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># We consider a 2-D problem</span>


<span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;D-dimensional Rosenbrock&#39;s banana function.&quot;&quot;&quot;</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">theta</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">100</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="c1"># In general, `log_likelihood` would depend on the data and *your* particular model</span>
</pre></div>
</div>
</div>
</div>
<p>We define now a prior <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span> over the parameters. For simplicity, we set an independent Gaussian prior on each variable, but you could use any prior of your choice. Note that PyVBMC uses the logarithm of the prior, <span class="math notranslate nohighlight">\(\log p(\mathbf{x})\)</span>.</p>
<p>Since in this example the priors for each variable are independent, we compute the log-prior separately for each variable and then sum them:</p>
<div class="math notranslate nohighlight">
\[
\log p(\mathbf{x}) = \log \prod_{d = 1}^{D} p(x_d) = \sum_{d = 1}^{D} \log p(x_d).
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prior_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">D</span><span class="p">))</span>
<span class="n">prior_std</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">D</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Independent normal prior.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">scs</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prior_mu</span><span class="p">,</span> <span class="n">prior_std</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>By default PyVBMC assumes that the provided <code class="docutils literal notranslate"><span class="pre">target</span></code> function is the log-joint, or unnormalized log posterior, defined as:</p>
<div class="math notranslate nohighlight">
\[
\log p(\mathcal{D}, \mathbf{x}) = \log p(\mathcal{D}| \mathbf{x}) p(\mathbf{x}) = \log p(\mathcal{D}| \mathbf{x}) + \log p(\mathbf{x}).
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_joint</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;log-density of the joint distribution.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">log_likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_prior</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="parameter-setup-bounds-and-starting-point">
<h2>2. Parameter setup: bounds and starting point<a class="headerlink" href="#parameter-setup-bounds-and-starting-point" title="Permalink to this headline">#</a></h2>
<p>We assume an unconstrained domain for the model parameters, that is <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^D\)</span>, specifying <code class="docutils literal notranslate"><span class="pre">-inf</span></code> and <code class="docutils literal notranslate"><span class="pre">inf</span></code> for the lower and upper bounds, respectively.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">LB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>  <span class="c1"># Lower bounds</span>
<span class="n">UB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>  <span class="c1"># Upper bounds</span>
</pre></div>
</div>
</div>
</div>
<p>PyVBMC also requires the user to specify so-called <em>plausible bounds</em> which should denote a region of high posterior probability mass. The plausible range is used to initialize some hyperparameters of the inference algorithm and guide the initial exploration of the posterior landscape, but it does not otherwise affect the model (although a good choice of plausible range can considerably improve convergence of the algorithm).</p>
<p>Not knowing better, we use mean +/- 1 SD of the prior (that is, the top ~68% prior credible interval) to set the plausible bounds.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">PLB</span> <span class="o">=</span> <span class="n">prior_mu</span> <span class="o">-</span> <span class="n">prior_std</span>  <span class="c1"># Plausible lower bounds</span>
<span class="n">PUB</span> <span class="o">=</span> <span class="n">prior_mu</span> <span class="o">+</span> <span class="n">prior_std</span>  <span class="c1"># Plausible upper bounds</span>

<span class="c1"># Alternatively, you could set the plausible bounds using the quantiles:</span>
<span class="c1"># PLB = scs.norm.ppf(0.1587, prior_mu, prior_std)</span>
<span class="c1"># PUB = scs.norm.ppf(0.8413, prior_mu, prior_std)</span>
</pre></div>
</div>
</div>
</div>
<p>As a starting point <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> for the algorithm, we use the mean of the prior:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">prior_mu</span><span class="p">)</span>

<span class="c1"># Alternatively, we could use a random sample from inside the plausible box:</span>
<span class="c1"># x0 = PLB + np.random.uniform(size=(1,D))*(PUB - PLB)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="initialize-and-run-inference">
<h2>3. Initialize and run inference<a class="headerlink" href="#initialize-and-run-inference" title="Permalink to this headline">#</a></h2>
<p>We now initialize the <code class="docutils literal notranslate"><span class="pre">vbmc</span></code> object which takes care of the inference. For now, we use default options.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vbmc</span> <span class="o">=</span> <span class="n">VBMC</span><span class="p">(</span><span class="n">log_joint</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">LB</span><span class="p">,</span> <span class="n">UB</span><span class="p">,</span> <span class="n">PLB</span><span class="p">,</span> <span class="n">PUB</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To perform inference, we run <code class="docutils literal notranslate"><span class="pre">vbmc.optimize()</span></code>.</p>
<p>The algorithm returns the variational posterior <code class="docutils literal notranslate"><span class="pre">vp</span></code> and a <code class="docutils literal notranslate"><span class="pre">results</span></code> dictionary containing additional information about the optimization, which we will analyze in more detail in a subsequent notebook.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vp</span><span class="p">,</span> <span class="n">results</span> <span class="o">=</span> <span class="n">vbmc</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Beginning variational optimization assuming EXACT observations of the log-joint.
 Iteration  f-count    Mean[ELBO]    Std[ELBO]    sKL-iter[q]   K[q]  Convergence  Action
     0         10          -5.55         3.66     32357.09        2        inf     start warm-up
     1         15          -2.93         0.07         2.35        2        inf     
     2         20          -2.80         0.01         0.03        2       1.05     
     3         25          -2.79         0.00         0.02        2      0.466     
     4         30          -2.79         0.00         0.01        2       0.19     end warm-up
     5         35          -2.78         0.00         0.00        2      0.109     
     6         40          -2.80         0.00         0.01        2      0.282     
     7         45          -2.52         0.00         0.20        5       5.71     
     8         50          -2.47         0.00         0.02        6      0.591     
     9         55          -2.44         0.00         0.01        9      0.381     rotoscale, undo rotoscale
    10         60          -2.38         0.00         0.00       12      0.238     
    11         65          -2.33         0.00         0.02       15      0.574     
    12         70          -2.31         0.00         0.01       17       0.32     
    13         75          -2.31         0.00         0.00       18        0.1     stable
   inf         75          -2.29         0.00         0.00       50        0.1     finalize
Inference terminated: variational solution stable for options.tol_stable_count fcn evaluations.
Estimated ELBO: -2.289 +/-0.000.
</pre></div>
</div>
</div>
</div>
<p>Two especially relevant statistics are the lower bound on the log model evidence (<code class="docutils literal notranslate"><span class="pre">results[&quot;elbo&quot;]</span></code>), and its uncertainty (<code class="docutils literal notranslate"><span class="pre">results[&quot;elbo_sd&quot;]</span></code>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lml_true</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.272</span>  <span class="c1"># ground truth, which we know for this toy scenario</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The true log model evidence is:&quot;</span><span class="p">,</span> <span class="n">lml_true</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The obtained ELBO is:&quot;</span><span class="p">,</span> <span class="nb">format</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;elbo&quot;</span><span class="p">],</span> <span class="s2">&quot;.3f&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The obtained ELBO_SD is:&quot;</span><span class="p">,</span> <span class="nb">format</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;elbo_sd&quot;</span><span class="p">],</span> <span class="s2">&quot;.3f&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The true log model evidence is: -2.272
The obtained ELBO is: -2.289
The obtained ELBO_SD is: 0.000
</pre></div>
</div>
</div>
</div>
<section id="remarks">
<h3>Remarks:<a class="headerlink" href="#remarks" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The ELBO is a <em>lower bound</em> on the true log model evidence <span class="math notranslate nohighlight">\(\log p(\mathcal{D})\)</span>. The better the approximation of the posterior, the closer the ELBO is to the true log model evidence.</p></li>
<li><p>PyVBMC does not aim for high numerical precision of the ELBO (e.g., beyond the 1st or 2nd decimal place). In most realistic model-fitting problems, a higher resolution is not necessary.</p></li>
<li><p>The reported standard deviation of the ELBO, <code class="docutils literal notranslate"><span class="pre">elbo_sd</span></code>, is a measure of the uncertainty on the ELBO as estimated via Bayesian quadrature (the approximation technique used by PyVBMC). <code class="docutils literal notranslate"><span class="pre">elbo_sd</span></code> is <strong>not</strong> a measure of the difference between the ELBO and the true log model evidence, which is generally unknown.</p></li>
</ul>
</section>
</section>
<section id="examine-and-visualize-results">
<h2>4. Examine and visualize results<a class="headerlink" href="#examine-and-visualize-results" title="Permalink to this headline">#</a></h2>
<p>We now examine the obtained variational posterior. We can easily draw hundreds of thousands of random samples from the variational posterior to compute summary statistics of interests.</p>
<p>For reporting uncertainty on model parameter estimates, you could use posterior mean +/- SD, or the median and interquartile range of the samples (the latter is better for a posterior that deviates substantially from Gaussian).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First, generate a large number of samples from the variational posterior:</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">3e5</span><span class="p">)</span>
<span class="n">Xs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">vp</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

<span class="c1"># Easily compute statistics such as moments, credible intervals, etc.</span>
<span class="n">post_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Xs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Posterior mean</span>
<span class="n">post_cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">Xs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>  <span class="c1"># Posterior covariance matrix</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The approximate posterior mean is:&quot;</span><span class="p">,</span> <span class="n">post_mean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The approximate posterior covariance matrix is:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">post_cov</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The approximate posterior mean is: [0.01283219 1.09903401]
The approximate posterior covariance matrix is:
 [[1.17328411 0.04476398]
 [0.04476398 1.78108106]]
</pre></div>
</div>
</div>
</div>
<p>Finally, we visualize the obtained variational approximation <code class="docutils literal notranslate"><span class="pre">vp</span></code> via the <code class="docutils literal notranslate"><span class="pre">vp.plot()</span></code> function, which is based on the beautiful <code class="docutils literal notranslate"><span class="pre">corner.py</span></code> module (see <a class="reference external" href="https://corner.readthedocs.io/en/latest/">here</a>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vp</span><span class="o">.</span><span class="n">plot</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/18626e57f76f7b5f3bfca8fe0928740b9c95270d269b60675d9bcd1af12cb9c7.png" src="../_images/18626e57f76f7b5f3bfca8fe0928740b9c95270d269b60675d9bcd1af12cb9c7.png" />
</div>
</div>
<p>The figure represents the multidimensional variational posterior <code class="docutils literal notranslate"><span class="pre">vp</span></code> via one- and two-dimensional marginal plots for each variable and pairs of variables.</p>
<p>Here, the bottom-left panel represents the two-dimensional joint distribution of <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>, whereas the two panels on the diagonal represent the one-dimensional marginal distributions of <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>, respectively. Note that PyVBMC has managed to approximate very well the highly non-Gaussian shape of the target density.</p>
</section>
<section id="conclusions">
<h2>5. Conclusions<a class="headerlink" href="#conclusions" title="Permalink to this headline">#</a></h2>
<p>In this notebook, we have seen how to set up and run Bayesian inference with PyVBMC with a fairly minimal example.</p>
<p>An important step which is not considered in this example is a thorough validation of the results and diagnostics, which will be discussed in a later notebook.</p>
<p>The next notebook will look more in detail at the output trace and plots of PyVBMC, which is a good starting point to check how inference is proceeding.</p>
</section>
<section id="acknowledgments">
<h2>Acknowledgments<a class="headerlink" href="#acknowledgments" title="Permalink to this headline">#</a></h2>
<p>Work on the PyVBMC package was funded by the <a class="reference external" href="https://fcai.fi/">Finnish Center for Artificial Intelligence FCAI</a>.</p>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../api/functions/whitening.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">whitening</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="pyvbmc_example_2_inputs_outputs.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">PyVBMC Example 2: Understanding the inputs and the output trace</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
      &copy; Copyright 2022, Machine and Human Intelligence research group (PI: Luigi Acerbi, University of Helsinki).<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>